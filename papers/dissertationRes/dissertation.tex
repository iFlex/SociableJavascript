\pdfoutput=1

\documentclass{l4proj}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{url}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{cleveref}
\usepackage{dirtree}
\usepackage{subfigure}
\usepackage[]{algorithm2e}
\usepackage{tabularx}

\hypersetup{
  colorlinks   = true, %Colours links instead of ugly boxes
  urlcolor     = blue, %Colour for external hyperlinks
  linkcolor    = blue, %Colour of internal links
  citecolor   = black %Colour of citations
}

\colorlet{punct}{red!60!black}
\definecolor{lightgray}{rgb}{0.95, 0.95, 0.95}
\definecolor{darkgray}{rgb}{0.4, 0.4, 0.4}
%\definecolor{purple}{rgb}{0.65, 0.12, 0.82}
\definecolor{editorGray}{rgb}{0.95, 0.95, 0.95}
\definecolor{editorOcher}{rgb}{1, 0.5, 0} % #FF7F00 -> rgb(239, 169, 0)
\definecolor{editorGreen}{rgb}{0, 0.5, 0} % #007C00 -> rgb(0, 124, 0)
\definecolor{orange}{rgb}{1,0.45,0.13}    
\definecolor{olive}{rgb}{0.17,0.59,0.20}
\definecolor{brown}{rgb}{0.69,0.31,0.31}
\definecolor{purple}{rgb}{0.38,0.18,0.81}
\definecolor{lightblue}{rgb}{0.1,0.57,0.7}
\definecolor{lightred}{rgb}{1,0.4,0.5}
\definecolor{background}{HTML}{EEEEEE}
\definecolor{delim}{RGB}{20,105,176}
\colorlet{numb}{magenta!60!black}
%V8 C++
\lstdefinelanguage{cpp}{
  morekeywords={typeof, new, true, false, catch, function, return, null, catch, switch, var, if, in, while, do, else, case, break,Isolate,CreateParams,New},
  morecomment=[s]{/*}{*/},
  morecomment=[l]//,
  morestring=[b]",
  morestring=[b]'
}
% CSS
\lstdefinelanguage{CSS}{
  keywords={color,background-image:,margin,padding,font,weight,display,position,top,left,right,bottom,list,style,border,size,white,space,min,width, transition:, transform:, transition-property, transition-duration, transition-timing-function}, 
  sensitive=true,
  morecomment=[l]{//},
  morecomment=[s]{/*}{*/},
  morestring=[b]',
  morestring=[b]",
  alsoletter={:},
  alsodigit={-}
}

% JavaScript
\lstdefinelanguage{JavaScript}{
  morekeywords={typeof, new, true, false, catch, function, return, null, catch, switch, var, if, in, while, do, else, case, break},
  morecomment=[s]{/*}{*/},
  morecomment=[l]//,
  morestring=[b]",
  morestring=[b]'
}

\lstdefinelanguage{HTML5}{
  language=html,
  sensitive=true, 
  alsoletter={<>=-},  
  morecomment=[s]{<!-}{-->},
  tag=[s],
  otherkeywords={
  % General
  >,
  % Standard tags
  <!DOCTYPE,
  </html, <html, <head, <title, </title, <style, </style, <link, </head, <meta, />,
  % body
  </body, <body,
  % Divs
  </div, <div, </div>, 
  % Paragraphs
  </p, <p, </p>,
  % scripts
  </script, <script,
  % More tags...
  <canvas, /canvas>, <svg, <rect, <animateTransform, </rect>, </svg>, <video, <source, <iframe, </iframe>, </video>, <image, </image>, <header, </header, <article, </article
  },
  ndkeywords={
  % General
  =,
  % HTML attributes
  charset=, src=, id=, width=, height=, style=, type=, rel=, href=,
  % SVG attributes
  fill=, attributeName=, begin=, dur=, from=, to=, poster=, controls=, x=, y=, repeatCount=, xlink:href=,
  % properties
  margin:, padding:, background-image:, border:, top:, left:, position:, width:, height:, margin-top:, margin-bottom:, font-size:, line-height:,
  % CSS3 properties
  transform:, -moz-transform:, -webkit-transform:,
  animation:, -webkit-animation:,
  transition:,  transition-duration:, transition-property:, transition-timing-function:,
  }
}
\lstdefinestyle{htmlcssjs} {%
  % General design
%  backgroundcolor=\color{editorGray},
  basicstyle={\tiny},   
  frame=b,
  % line-numbers
  xleftmargin={0.75cm},
  numbers=left,
  stepnumber=1,
  firstnumber=1,
  numberfirstline=true, 
  % Code design
  identifierstyle=\color{black},
  keywordstyle=\color{blue}\bfseries,
  ndkeywordstyle=\color{editorGreen}\bfseries,
  stringstyle=\color{editorOcher}\ttfamily,
  commentstyle=\color{brown}\ttfamily,
  % Code
  language=HTML5,
  alsolanguage=JavaScript,
  alsodigit={.:;},  
  tabsize=2,
  showtabs=false,
  showspaces=false,
  showstringspaces=false,
  extendedchars=true,
  breaklines=true,
  % German umlauts
  literate=%
  {Ö}{{\"O}}1
  {Ä}{{\"A}}1
  {Ü}{{\"U}}1
  {ß}{{\ss}}1
  {ü}{{\"u}}1
  {ä}{{\"a}}1
  {ö}{{\"o}}1
}
%
\lstdefinelanguage{json}{
    basicstyle=\normalfont\ttfamily,
    numbers=left,
    numberstyle=\scriptsize,
    stepnumber=1,
    numbersep=8pt,
    showstringspaces=false,
    breaklines=true,
    frame=lines,
    backgroundcolor=\color{background},
    literate=
     *{0}{{{\color{numb}0}}}{1}
      {1}{{{\color{numb}1}}}{1}
      {2}{{{\color{numb}2}}}{1}
      {3}{{{\color{numb}3}}}{1}
      {4}{{{\color{numb}4}}}{1}
      {5}{{{\color{numb}5}}}{1}
      {6}{{{\color{numb}6}}}{1}
      {7}{{{\color{numb}7}}}{1}
      {8}{{{\color{numb}8}}}{1}
      {9}{{{\color{numb}9}}}{1}
      {:}{{{\color{punct}{:}}}}{1}
      {,}{{{\color{punct}{,}}}}{1}
      {\{}{{{\color{delim}{\{}}}}{1}
      {\}}{{{\color{delim}{\}}}}}{1}
      {[}{{{\color{delim}{[}}}}{1}
      {]}{{{\color{delim}{]}}}}{1},
}
\definecolor{mygreen}{rgb}{0,0.6,0}
\definecolor{mygray}{rgb}{0.5,0.5,0.5}
\definecolor{mymauve}{rgb}{0.58,0,0.82}

\lstset{ %
  backgroundcolor=\color{white},   % choose the background color; you must add \usepackage{color} or \usepackage{xcolor}
  basicstyle=\footnotesize,        % the size of the fonts that are used for the code
  breakatwhitespace=false,         % sets if automatic breaks should only happen at whitespace
  breaklines=true,                 % sets automatic line breaking
  captionpos=b,                    % sets the caption-position to bottom
  commentstyle=\color{mygreen},    % comment style
  deletekeywords={...},            % if you want to delete keywords from the given language
  escapeinside={\%*}{*)},          % if you want to add LaTeX within your code
  extendedchars=true,              % lets you use non-ASCII characters; for 8-bits encodings only, does not work with UTF-8
  frame=single,                    % adds a frame around the code
  keepspaces=true,                 % keeps spaces in text, useful for keeping indentation of code (possibly needs columns=flexible)
  keywordstyle=\color{blue},       % keyword style
  language=Octave,                 % the language of the code
  otherkeywords={*,...},           % if you want to add more keywords to the set
  numbers=left,                    % where to put the line-numbers; possible values are (none, left, right)
  numbersep=5pt,                   % how far the line-numbers are from the code
  numberstyle=\tiny\color{mygray}, % the style that is used for the line-numbers
  rulecolor=\color{black},         % if not set, the frame-color may be changed on line-breaks within not-black text (e.g. comments (green here))
  showspaces=false,                % show spaces everywhere adding particular underscores; it overrides 'showstringspaces'
  showstringspaces=false,          % underline spaces within strings only
  showtabs=false,                  % show tabs within strings adding particular underscores
  stepnumber=2,                    % the step between two line-numbers. If it's 1, each line will be numbered
  stringstyle=\color{mymauve},     % string literal style
  tabsize=2,                     % sets default tabsize to 2 spaces
  title=\lstname                   % show the filename of files included with \lstinputlisting; also try caption instead of title
}
\begin{document}
\title{Sociable Javascript}
\author{Milorad Liviu Felix}
\date{\today}
\maketitle
\educationalconsent
%
%NOTE: if you include the educationalconsent (above) and your project is graded an A then
%      it may be entered in the CS Hall of Fame
%
\newpage
\begin{abstract}
   As \textbf{JavaScript} has grown in popularity, it has also become more versatile. Currently, JavaScript is one of the fastest scripting languages available, however, memory is not handled optimally. A large number of applications running JavaScript on the same machine may result in greater memory contention among processes. To make the best use of system resources while maximising utilisation, the applications would need to be aware of each other's memory needs, and cooperate in order for them to continuously have access to the minimum running requirements. To achieve system-wide awareness at an individual application level, we propose a client-server model, where a manager process keeps track of all running client applications and controls how much memory they are allocated. We present a set of policies based on social welfare theory that model JavaScript execution environments as individuals part of a community and evaluate how they impact the execution speed and memory footprint of each application. An application with a low memory footprint, that has acceptable performance, could make JavaScript a suitable choice for elastic systems.
\end{abstract}
\newpage
\tableofcontents
\newpage
\pagenumbering{arabic}
\chapter{Introduction}
%intro
\hspace*{1em} \textbf{JavaScript} is a programming language for web-based applications. It began as a simple solution for animating web-page content, but has evolved into a \textbf{multi-purpose scripting language} with growing support and popularity. A large part of the functionality of web products is now implemented in JavaScript and there are initiatives, such as NodeJS\cite{nodejs}, that aim to bring this language to the \textbf{server-side}. Since the language is interpretive, browser vendors implement JavaScript \textbf{virtual machines} to execute the code. Examples of the most widely used JavaScript virtual machines are \textbf{V8} from Google Chrome\cite{v8}, Nitro from Apple Safari, Spidermonkey\cite{spidermk} from Mozilla Firefox and Chakra\cite{chakra} from Microsoft Edge.
\\\\%garbage collection
\hspace*{1em} One reason for the \textbf{considerable popularity} of the language is its \textbf{simplicity}. A key factor of this simplicity is that JavaScript programmers do not have to handle memory management. This allows developers to focus on the high level functionality, such as how an application responds to a certain request, rather than on low level implementation details, such as how much memory a new object needs and when that memory should be freed. The responsibility of memory management is delegated to the virtual machine. While \textbf{memory allocation} may be straight forward (a new block of memory is requested from the host operating system when the application creates a new object), \textbf{freeing memory} cannot be done in the same manner, as an object that goes out of scope might still be referenced by a \textbf{live object} (object currently used by the application). The virtual machine needs to inspect the live objects, detect which ones have truly expired and free the memory they occupy. This process is called \textbf{garbage collection}.%k

\section{Virtual Machine Optimisation}
\hspace*{1em} JavaScript virtual machines are \textbf{optimised} to offer the best possible performance from a user perspective, this means that \textbf{speed} of execution and \textbf{steady rendering} are most important. In the case of V8, the engineers that designed it are using the term \textbf{``jank''}\cite{jank} to refer to noticeable rendering pauses caused by JavaScript garbage collection and are currently striving to minimise it. One example of jank is when the user scrolls the page and the animation has noticeable pauses that make it look \textbf{discontinuous}. In order to carry out garbage collection, the execution of the JavaScript application needs to be paused, which can cause jank to occur. To reduce the impact observable by the user, garbage collection work is divided into small segments and interleaved with longer application execution sequences. This improves the user experience, but increases the memory footprint of the application as unused memory is held for longer than necessary. Such a system is not concerned with conserving memory and cooperating with other processes in order to allow as many applications as possible to run on the system, but this behaviour is justified as each tab in a web-browser needs to display as smoothly as possible.%k
\\\\
\section{Multi-purpose JavaScript}
\hspace*{1em} \textbf{JavaScript has outgrown its original intended purpose} and has expanded to server-side and client-side applications (both desktop and mobile). In the case of server-side applications, the \textbf{NodsJS} framework is experiencing a \textbf{growing adoption} trend for services such as web-servers, push notification servers, server-side voice recognition and other computation bound services. This is in part due to the fact that a large majority of the JavaScript library ecosystem is compatible with NodeJS, leaving out only the libraries that utilise HTML document related methods and structures (which are not present in NodeJS). In this case, jank does not exist since no rendering is done. Therefore, garbage collection would only impact total execution time. Service providers using JavaScript would greatly benefit if their applications had a \textbf{lower memory footprint}, as more processes can be run per physical machine, \textbf{increasing service availability}. Being able to limit the memory expansion of garbage collected applications guarantees that system processes would always have enough memory to remain responsive, regardless of how loaded the client serving applications are. \textbf{Elastic systems} can benefit from a service that controls the memory usage of garbage collected applications, by dynamically adapting it (memory limits tightened or relaxed) depending on the load and total amount of memory made available at each point in time.%k
\\\\
\hspace*{1em} JavaScript \textbf{client-side applications} are also beginning to emerge in the form of NWJS (Node Webkit JavaScript) applications. In essence, this framework is a web browser rendering a local web-page which has access to JavaScript file system APIs and other native application functionality that is normally not included in regular web browsers for security reasons. This approach is gaining popularity especially among organisations that lack the human resources necessary to build separate native applications for each platform they intend to support, or want to deploy their existing web applications in the form of native programs. 
Examples of such applications are: Powder Player - a video streaming application with torrent download and seeding integration\cite{powderplayer}, WhatsApp - a popular messenger application acquired by Facebook in 2014 \cite{whatsap}, Facebook messenger - Facebook's own messenger platform \cite{messenger}, DevKit - a modular integrated development environment \cite{devkit},
Wunderlist - a popular productivity application allowing users to create and share task lists\cite{wunderlist} and even a game named GameDev Tycoon\cite{tycoongame}. Running many such \textbf{large applications concurrently} could result in \textbf{memory contention} due to the way JavaScript virtual machines handle memory. As a consequence, the overall performance of the applications would decrease, affecting their quality of service. Having a mechanism for optimising the memory usage of JavaScript could solve this issue, allowing users to run many such applications concurrently.%K 

\section{Project Scope}
\hspace*{1em} The purpose of this project is to build a framework capable of \textbf{monitoring the memory usage} of every JavaScript application within a certain environment (home computer, or cluster of servers running JavaScript powered services) and \textbf{limiting} their \textbf{memory footprints}. Each application would receive a heap size limit which would then be enforced by the JavaScript virtual machine. This would be achieved by freeing more memory during garbage collection in order to comply with the limit. %k
\\\\ %motivation and social welfare
%(REPHRASE)The motivation for building such a system is improving the maximum possible \textbf{multi-programming degree}, availability and fault tolerance for services based on JavaScript applications, making this technology suitable for \textbf{multi-tasking and elastic systems}. 
\hspace*{1em} The \textbf{main benefit} of enforcing a global memory management policy over all of the JavaScript runtimes on a machine is \textbf{reducing competition for resources and increasing cooperation}. Garbage collection will be done more frequently, thus freeing up more memory and allowing more applications to run on the same machine. The end goal would be to increase the multi-programming degree without hindering performance.\\\\ %k
\hspace*{1em} Through the coordinated management based on global information, applications achieve a form of cooperation much like individuals cooperate and responsibly share resources in order to maintain a necessary level of comfort. The key factor to the success of this framework would be to reduce the memory footprint while maintain acceptable performance levels for each JavaScript application. Performance translates directly to how long each program takes to complete. If the execution time increases considerably, the applications become infeasible. In order to find a balance between performance and memory size, the framework models execution contexts as \textbf{individual members of a community} and applies a \textbf{social welfare function} in order to determine how much memory each individual is allowed to use. We attempt to build such a cooperative memory manager using concepts from \textbf{social welfare theory}.
\\\\
This project is comprised of two main components:
\begin{enumerate}
\item A modified \textbf{JavaScript virtual machine} - executes JavaScript, calculates and reports \textbf{runtime metrics} (heap size, throughput, etc) and enforces memory limits received from the manager.
\item A \textbf{manager framework} - receives metrics from monitored virtual machines, \textbf{applies a management policy} and issues limits to each application (virtual machine).
\end{enumerate}

The manager is a Python application and the virtual machine selected for this project is Google's V8 engine. Figure \ref{overviewfig} shows an overview of system.
\begin{figure}[!ht]
  \centering
    \includegraphics[width=0.75\textwidth]{SimplifiedOverall.png}
    \caption{Overall project diagram.}
    \label{overviewfig}
\end{figure}

The approach adopted was similar to the Java Forseti system developed at the University of Glasgow\cite{forseti}. The V8 engine was modified to allow dynamic expanding and shrinking of a application's heap size and a network client allowing interaction with the manager was added. This was a challenging task since the V8 project consists of \textbf{800,000+ lines of code}.

\section{Project Outline}

The structure of this report is outlined below.\\\\
\textbf{Chapter 2 - Background:}
Background information\\
\textbf{Chapter 3 - Requirements:}
Functional and non-functional requirements of the management framework.\\
\textbf{Chapter 4 - Design:}
Design of the management framework and required V8 engine modifications.\\
\textbf{Chapter 5 - Implementation}
Implementation of the management framework and V8 engine modifications.\\
\textbf{Chapter 6 - Evaluation:}
Empirical evaluation of the framework effectiveness when using various memory management policies.\\
\textbf{Chapter 7 - Conclusions}
Conclusions and future advances.%k

\chapter{Background}
\hspace*{1em} This chapter presents background information related to the project as well as related work and terminology used throughout the rest of the report.
\section{Welfare Economics}
\hspace*{1em} ``\textit{\textbf{Welfare economics} is a branch of economics that uses microeconomic techniques to evaluate well-being (welfare) at the aggregate (economy-wide) level}"\cite{welfareeconomics}. The typical evaluation begins with the derivation of a \textbf{social welfare function}. In welfare economics, a social welfare function ranks social states (alternative complete descriptions of the society) as less desirable, more desirable, or indifferent for every possible pair of social states. Inputs of the function include any variables considered to affect the economic welfare of a society\cite{socialwelfarefunction}. This is then used to rank economically feasible allocations of resources. JavaScript applications can be modelled as individuals using a combination of the runtime metrics as input values for the social welfare function. \textit{The exact choice of the social welfare function, selected set of runtime metrics and the way they are mapped to the input values of the function are what we call a \textbf{policy}}. This project aims to evaluate the effectiveness of various policies in relation to different types of JavaScript applications.
\section{JavaScript}
\hspace*{1em} In 1994, \textbf{Netscape} developed a web-browser that was meant to exploit the potential of the emerging \textbf{World Wide Web}, its name was Netscape Navigator. The engineers behind the browser quickly realized that the Web needed to be more dynamic as even basic input validation had to be done by the server (requiring the browser to send the data over the network to the server and receive feedback). Later in 1995, a debate began among the engineers at Netscape about whether to add a static or scripting language to their browser. The proponents of a scripting language offered the following explanation: 
\textit{``We aimed to provide a “glue language” for the Web designers and part time programmers who were building Web content from components such as images, plugins, and Java applets. We saw Java as the “component language” used by higher-priced programmers, where the glue programmers—the Web page designers—would assemble components and automate their interactions using a scripting language''}\cite{jsgrandpa}.
Netscape management had decided that a scripting language had to have a syntax similar to Java’s largely because of their collaboration with Sun, the company that created the Java programming language. In late November 1995, Navigator 2.0B3 was released and included the JavaScript prototype, which continued its early existence without major changes. In early \textbf{December 1995}, the language's momentum had grown and it was renamed, to its final name, \textbf{JavaScript}\cite{jsdaddy}.
\\\\
\hspace*{1em} JavaScript has since grown to be one of the most popular programming languages. It has become the most prevalent language on  GitHub\cite{github} (which in turn is one of the most popular version control services) as illustrated by figure \ref{jspop}.

\begin{figure}[!ht]
\centering
\subfigure[JavaScript on GitHub in 2014 \cite{githut}]{\includegraphics[height=4.5cm]{JsPop}}
\subfigure[2016 RedMonk popularity rank \cite{monk}]{\includegraphics[height=4.5cm]{2016Pop}}
\caption{JavaScript popularity graph}
\label{jspop}
\end{figure}

The popularity of a programming language comes from the size and activity of its community (the developers that build projects with the said language) and this means that JavaScript has a vast and active developer base.

\section{Garbage Collection}
\label{gcexplained}
\hspace*{1em}\textbf{Garbage collection} is the process of automatically identifying and \textbf{reclaiming unused memory segments}. In many systems, programmers need to explicitly specify when memory should be reclaimed using predefined programming language constructs such as ``free'' or ``delete''. Automatic memory management (garbage collection) fully assumes this responsibility in order for the programmer to better focus on implementing program functionality rather than memory management. This is done by traversing the pointer relation graph in order to distinguish \textit{live} objects (in use objects) from \textit{garbage} (objects no longer in use). Memory occupied by garbage is either returned to the operating system or reused by the allocation mechanism of the running application\cite{gcpaper}.%k
\begin{figure}[!ht]
  \centering
    \includegraphics[height=15em]{Heap.png}
    \caption{Basic Memory System Diagram}
\end{figure}
\\\\
\hspace*{1em} V8 uses a \textbf{generational garbage collector} which divides the heap in two memory spaces: new-space and old-space, perceived as two generations. Objects are allocated in new-space and if they survive (are not collected) for a certain amount of time, they are promoted (moved) to the old-space. The new-space is collected much more often than the old-space. There are three main algorithms used to manage these two spaces: \textbf{copying garbage collection} using the Cheney traversal algorithm\cite{cheney}, \textbf{mark-sweep} and \textbf{mark-compact}. The first one handles the new-space while the other two handle the old-space. 
\\\\
Mark-sweep is named after its two comprising phases, presented by Wilson et al. in the Uniprocessor Garbage Collection Techniques research paper\cite{gcpaper}: 
\begin{enumerate}
\item Mark - distinguish the live objects from garbage by traversing the pointer relationship graph starting at the root set. Either using depth first or breadth first traversal, objects that are reached are marked in some way.

\item Sweep - reclaiming the garbage by sweeping the memory in an exhaustive manner in order to find all of the unmarked objects and reclaim their space. The reclaimed objects are linked onto a set of free lists in order to make them accessible to the allocation routines.
\end{enumerate}
There are two major problems for this type of garbage collection as presented in the same paper:
\begin{enumerate}
\item Handling objects of varying size without fragmentation of available memory is difficult. Even with the addition of a number of free lists and joining adjacent free spaces in order to fit a large object, difficulties still arise. There may not be enough smaller adjacent spaces to produce a contiguous memory sector that fits a new large object. 
\item Locality of reference is a second problem. After a collection, live objects are interleaved with free spaces since memory is never moved. New objects are allocated in these free spaces, which creates localities with objects that do not have similar ages.%k
\end{enumerate}
\hspace*{1em} Mark-compact remedies the fragmentation issue of mark-sweep by compacting the memory it sweeps. As with mark-sweep, this algorithm traverses the graph of pointer relations and marks the live objects. Then objects are traversed and live objects are moved until they form a contiguous section of memory, leaving the rest of the free memory in a single contiguous free space. Compacting the memory into 2 contiguous spaces (live objects and free memory) has two main benefits: handling variable size objects is greatly simplified and locality of reference is strengthened. The main disadvantage of mark-compact, when compared to mark-sweep, is that moving objects in memory is more time consuming and therefore makes is slower. However, mark-compact brings considerable improvements to the execution speed of the host applications because of strong locality of reference and simple allocation of variable size objects\cite{gcpaper}.%k
\\\\
\hspace*{1em} ``Like mark-compact (but unlike mark-sweep), copying garbage collection does not really collect garbage. Rather, it moves all live objects into one area, and the rest of the heap is then known to be available because it contains only garbage'' \cite{gcpaper} V8 uses a ``stop-and-copy'' copying garbage collector with the Cheney traversal algorithm for its new-space called \textbf{Scavenger}. This algorithm divides the new-space in two equally sized halves of contiguous memory. Only one is in use while the application is running, the other is used for the compaction of memory. This makes allocation very fast as it only requires incrementing a pointer to the space in use. When allocation no longer fits the currently used half, collection is performed and all the live objects are copied into the other half, which causes the them to switch roles\cite{gcpaper}. This kind of collection is well suited for small spaces that contain short lived objects as it is fast and maintains locality of reference.
\section{The V8 Engine}
\hspace*{1em} V8 is an open source, high-performance JavaScript engine developed by Google. It is written in C++ and used in Google Chrome which is the open source browser from Google. It runs on Windows XP or later, Mac OS X 10.5+, and Linux systems that use IA-32, ARM or MIPS processors. V8 can run as a standalone program, or can be embedded into any C++ application\cite{v8}.
\\\\
\hspace*{1em} V8 compiles JavaScript to native machine code in an attempt to maximise performance. It uses a generational garbage collector with two generations: \textit{new-space} and \textit{old-space}. Objects surviving the new-space for a certain amount of time are promoted to the old-space.
\\\\
\hspace*{1em} Multiple JavaScript applications can be run by the same V8 process. In order to preserve security, applications are isolated from each other. An application cannot access memory it does not own even though the operating system is not aware that multiple applications are running inside a single V8 process. An \textbf{isolate} is an instance of the V8 execution environment capable of running one JavaScript application. Each isolate has its own heap, garbage collector, compiled code and can not interact with any other isolate. This term will be used to refer to a JavaScript application.
\\\\
\hspace*{1em} An applications is normally comprised of multiple JavaScript source files. V8 uses \textbf{contexts} to represent them. One context represents a single compiled JavaScript source file. One or more contexts can be loaded in an isolate and will share the global scope as a result. 
\\\\
\hspace*{1em} V8 has become one of the most popular JavaScript engines and is part of many other open source initiatives that strive to bring this programming language into new fields such as: server-side scripting (NodeJS), client-side applications (NWJS), mobile phone applications (Titanium Studio).

\section{Why control resource allocation?}

\hspace*{1em} To better portray the benefit of controlling resource allocation, consider an elastic data centre running JavaScript applications to service various client requests as well as administrative tasks. The elastic nature of the environment would require increasing or decreasing the number of active host machines as well as the number of active servicing application depending on the load. 
\\\\
\hspace*{1em} Without any supervision, JavaScript applications would simply use as much memory as necessary to maximise performance. This would be done ``selfishly'' without consideration for the other processes running on the host machine. Performance is maximised by keeping garbage collection pauses as short and far apart as possible, causing the program to retain more memory than it actually needs. This would not pose any problems in situations where there is plenty of memory, but as the number of \textbf{processes running in parallel} increases so does the \textbf{memory contention} and the \textbf{risk of paging}.
\\\\
\hspace*{1em} In the situation where the available memory is nearly depleted, newly started applications would suffer from memory starvation causing them to run with very low performance or even fail. Existing applications could also suffer as their needs could change over time (a section of code that is more memory intensive executes), as there is no way to coordinate the JavaScript run-time environments in order to maximise the performance of every application by forcing some to relinquish more memory and giving it to the ones in need. In a situation with no available memory, applications would start thrashing which would gravely impact the overall performance of the system. In this scenario, if the system administrator did not reserve an amount of memory for system processes, the operation of the system would slow down considerably or even halt. 
\\\\
\hspace*{1em} With no supervising framework, the system administrator would have to estimate how many applications can run on a machine, depending on the amount of memory each machine has, and enforce such a limit on every machine in the environment. This solution is not viable for a modern elastic environment where the machine count can reach tens of thousands and new machines are added or removed dynamically depending on system load. 
\\\\
\hspace*{1em} A manager framework would \textbf{calculate and enforce a memory limit} for every JavaScript application it manages in an effort to \textbf{maximise the overall performance} of the environment as well as individual application performance. Managed applications would not be allowed to retain unused memory if there are other applications that need more memory. In contrast with the situation presented above, where no supervisor was present, portions of memory are taken from carefully selected applications and given to the newly started or under-performing programs. The framework can also be configured to assign limits within a certain threshold (budget) in order for the total memory consumption of JavaScript programs to be lower than the total physical memory. This would offer the guarantee that system processes have a dedicated portion of memory at all times and thrashing is avoided.
\\\\
\hspace*{1em} A JavaScript program would fail only if its assigned memory limit becomes lower than its minimum heap size. Since this size is considerably smaller than the unrestricted memory usage (see appendix \ref{minheapsize}), more applications could safely run on the same machine than in the scenario where no supervisor is present. This would increase the productivity and responsiveness potential of a host machine. Knowing the minimum heap size of a program could also enable the framework to perform an acceptance test for new applications starting on host a machine running N concurrent applications:
\begin{equation} 
\textit{AllocatableMemory} = \textit{TotalAllocatableMemory} - \sum_{i=1}^{N}\textit{MinimumHeapSize}_i
\end{equation}
If the value of the expression above is lower than the minimum heap size, the new application can not be allowed to start as it is highly likely to fail. Using this check a machine capable of hosting the new application could be found. However, in Section \ref{noheapsize} we discuss why finding the minimum heap size of real applications is impractical. 
\\\\
\hspace*{1em} A manager framework uses \textbf{insight provided by the running applications} to make resource allocation decisions. In a normal set-up, this insight is not needed since applications are serviced on a ``\textit{first come first served}'' basis. This allows any individual application to make demands best suited for its well being without concerning itself with the status of the others. Having insight provided by every running application has the main advantage of allowing a supervisor to work towards the \textbf{well being of the group} rather than of an individual. This way, memory is used more efficiently, more applications can run at the same time and newly started ones are less likely to suffer from memory starvation (the supervisor would redistribute the memory to accommodate the newly started ones).
\section{Related Work}
\subsection{Runtime Heap Resizing}
\hspace*{1em} The Java runtime environment implements a set of platform dependent policies for tuning garbage collection in order to meet a certain set of goals. When running a Java application tuning goals can be specified using command line parameters. 
\\\\
There are three kinds of goals:
\begin{enumerate}
\item Maximum Pause Time Goal - ``The pause time is the duration during which the garbage collector stops the application and recovers space that is no longer in use. The intent of the maximum pause time goal is to limit the longest of these pauses. An average time for pauses and a variance on that average is maintained by the garbage collector. The average is taken from the start of the execution but is weighted so that more recent pauses count more heavily. If the average plus the variance of the pause times is greater than the maximum pause time goal, then the garbage collector considers that the goal is not being met.''\cite{ergonomics}
\item Throughput Goal - ``The throughput goal is measured in terms of the time spent collecting garbage and the time spent outside of garbage collection (referred to as application time). If the throughput goal is not being met, then the sizes of the generations are increased in an effort to increase the time that the application can run between collections.''\cite{ergonomics}
\item Footprint Goal - ``If the throughput and maximum pause time goals have been met, then the garbage collector reduces the size of the heap until one of the goals (invariably the throughput goal) cannot be met. The goal that is not being met is then addressed.''\cite{ergonomics}
\end{enumerate}
This allows the user to choose between making garbage collection more predictable or maximising the throughput of the application or a combination of both. This behaviour is built into the Oracle HotSpot virtual machine in order to make Java more versatile. A real-time system would require garbage collection pauses to be predictable for successful process scheduling, the maximum pause time goal could be used satisfy this constraint, hence making Java suitable for such an environment. However, this approach is applied locally, on a per-application basis and is limited to Java application. Our manager framework aims to be a proof-of-concept, generic, memory management facility capable of managing heterogeneous systems. 

\subsection{Economic Models for Resource Sharing}
\label{forsetitalk}
\hspace*{1em} The inspiration for this project comes from research carried out at the University of Glasgow aiming to dynamically re-size the heap memory of multiple Java rutimes\cite{forseti}. This paper introduced the Forseti system as a \textit{holistic memory management} framework which allows a system administrator to specify the amount of memory to be shared between all the concurrent runtimes (Java virtual machines) on a physical system. Forseti manipulates the system of applications based on the relationship between throughput and heap size. Control over the heap size is achieved through modifications done to the Java runtime, which allow the framework to send memory recommendations to each application. These recommendations are then enforced by the runtime environments. Through these recommendations, the framework aims to maximise the the combined throughput of the set of virtual machines using concepts of economic utility.     
\\\\
Application throughput is calculated using the following equation:

\begin{equation}
throughput = \dfrac{mutationTime}{mutationTime + garbageCollectionTime}
\end{equation}

The relationship between throughput and heap size is equivalent to a utility function. This is an increasing function but with a slow growth factor, which is equivalent to having \textbf{diminishing returns as more heap memory is made available} to the application\cite{forseti}. Figure \ref{diminishret} shows an illustration of this function.
\begin{figure}[!ht]
  \centering
    \includegraphics[height=4cm,keepaspectratio]{diminishreturns.png}
    \caption{Utility function with diminishing returns\cite{forseti}}
    \label{diminishret}
\end{figure}\\
\hspace*{1em} The utility functions of the Java runtimes are combined to obtain the utility function of the entire system using multiplication:
\begin{equation}
T_\text{overall}(h_1,...,h_N) = \prod_\text{i=1}^\text{N}T_i(h_i) 
\end{equation}
T stands for throughput, N is the total number of runtimes (equivalent to the total number of heaps). The framework then maximises this function within the constraint set by the observer (system administrator). The constraint applied here is the total amount of memory intended for the entire system to use. An additional constraint not present in economics is the minimum heap size of an application. Programs require a certain minimum heap size to run at all, the framework must make sure every application receives at least its required minimum heap size\cite{forseti}.
\\\\
\hspace*{1em} The system is comprised of two parts: the Forseti daemon which calculates maximum heap size recommendations and a Java virtual machine modified to interact with the daemon. The Forseti daemon runs concurrently with the Java runtimes on the same machine. It observes their execution behaviour and calculates heap size recommendations, aiming to improve the overall throughput of the system. The total memory allowance for the daemon is set at launch time and all communication between it and the runtimes uses Unix sockets\cite{forseti}.
\\\\
\hspace*{1em} The results of this research are promising, the memory footprint was reduced up to 5 times without compromising application throughput. However, this relies on knowing the minimum heap size of the managed applications. 
\section{Minimum Heap Size vs. Behaviour Observation}
\label{noheapsize}
\hspace*{1em} Knowing the minimum heap size of the running applications in a real environment is a luxury. Applications depend on external parameters which change the memory utilisation, external events such as user or network events trigger the execution of different application branches with different memory needs. Finding the minimum heap size requires triggering such events in a way that simulates a worst case scenario (where memory utilisation is maximal) for the result to be accurate. Finding the event combination and subsequently the heap size can be very time consuming. 
\\\\
\hspace*{1em}  In the case of JavaScript and websites, the application is downloaded from the internet and the minimum heap size is unknown as well as the set of events that trigger different branches of the application. JavaScript's ability to download, compile and execute additional code as part of an already running application add to the complexity of this task. When websites are loaded, users do not expect animations to start after variable delays that would allow the runtime environment to discover the minimum heap size.
\\\\
\hspace*{1em} This project aims to estimate when an application is struggling to execute (does not have enough memory) by observing its behaviour and try to compensate by giving it more memory. We consider this approach to be more practical because it can be applied to any real-world application without the need of prior examination of each managed application. Since the heap size is unknown to the managing framework, the evaluation of this project focuses more on the survival rate of applications in a memory constraint environment rather than just on the footprint reduction and time cost incurred.
\section{Terminology}
\begin{itemize}
\item \textit{V8} - the open source Google JavaScript virtual machine
\item \textit{isolate} - an execution environment part of a V8 process with its own heap and garbage collector
\item \textit{metrics}, \textit{status updates}, \textit{status information} - isolate status information reflecting its memory usage and performance
\item \textit{manager}, \textit{framework}, \textit{manager framework}, \textit{supervisor}, \textit{supervisor framework} - the program responsible with controlling the memory utilisation of JavaScript applications.
\item \textit{budget} - the amount of memory the the framework can distribute among isolates running on the same machine
\end{itemize}
\chapter{Requirements}
\hspace*{1em} This chapter outlines the functional and non-functional requirements of the framework built as part of this project. These refer to both the manager process responsible with controlling memory usage as well as the V8 engine modifications.
\section{Functional Requirements}
\hspace*{1em} The functional requirements of the manager framework were prioritised using the MoSCoW model and are presented below.
\begin{enumerate}
\item Detect new V8 processes and add them to a pool of managed processes. \\| \textbf{Must have}

\item Detect when new isolates are created by their host V8 processes and add them to a pool of active isolates. \\| \textbf{Must have}

\item Detect when isolates finish execution and remove them from the pool of active isolates. \\| \textbf{Must have}

\item Detect when V8 processes finish execution, remove all isolates they host from the pool of active isolates and remove the process form the pool of managed processes. \\| \textbf{Must have}

\item Poll for metric updates and calculate memory limits, at a set frequency, for all active isolates. \\| \textbf{Must have}

\item For each managed isolate, save all its metrics to a CSV file for post-execution analysis. \\| \textbf{Must have}

\item For each managed isolate, plot a selection of its metrics on screen as the isolate executes.\\| \textbf{Should have}

\item Allow the user to change the polling frequency of the manager at runtime.\\| \textbf{Should have}

\item Allow the user to set memory limits for active isolates.\\| \textbf{Should have}

\item Allow the user to set machine memory budgets at runtime.\\| \textbf{Should have}

\item Allow the user to change the memory management policy at runtime.\\| \textbf{Could have}

\item Support configuration files applied at framework start-up.\\| \textbf{Could have}

\item Allow the user to configure the feedback system's behaviour at runtime to set whether CSV history files should be created, PNG files should be created or live plotting should be done.\\| \textbf{Could have}

\item Allow the user to run automated testing scenarios to evaluate the existing policies.\\| \textbf{Would be nice to have}

\item Allow the user to disable the feedback system so that the application only enforces memory limits. \\| \textbf{Would be nice to have}
\end{enumerate}

\section{Non-Functional Requirements}
\hspace*{1em} The non-functional requirements of the manager framework are presented below.
\begin{center}
\begin{tabular}{  | l | l | }
\hline  
  Requirement Description & Judgement Criteria \\
\hline
 1. Service up to 250,000 V8 processes within a 100ms time interval. & Performance\\
 2. Sum of all limits issued for a machine must not exceed its assigned budget & Correctness. \\
 3. Errors encountered within the framework should not lead to service interruption or termination. & Robustness \\
 4. Errors generated by policy scripts should not lead to service interruption or termination. & Robustness \\
 5. New policies should be added without framework code changes. & Extensibility \\
\hline
\end{tabular}
\end{center}

\chapter{Design}
\hspace*{1em} This chapter presents the design of the manager framework and of the V8 modifications. The framework is described in terms of its constituent components and how they interact while the V8 modifications are classified into modifications and additions.
\section{Overview}

The overall framework has two main components:
\begin{itemize}
\item A \textbf{manager} - keeps track of active V8 isolates, polls them for status information (metrics) periodically, groups and plots the status information, calculates appropriate memory limits and sends them to the corresponding isolates.
\item A \textbf{modified V8 engine} - connects to the manager, measures status information (metrics) and responds to poll requests and commands from the manager, enforces received memory limits by either intensifying or relaxing garbage collection.
\end{itemize}

\begin{figure}[!ht]
  \centering
    \includegraphics[width=0.85\textwidth]{OverallFramework.png}
  \caption{Overall project diagram.}
\end{figure}

\hspace*{1em} In distributed systems a high per node multi-programming degree translates to high availability. This kind of environment also has potential to behave in an elastic way, making more resources available when the load increases and reducing the utilised resources when it diminishes. Our framework would have a greater impact when applied to a cluster of computers running JavaScript applications rather than a single computer.
\\\\
\hspace*{1em} In order to accommodate this scenario, the interaction between the V8 instances and the manager is done through the network, using \textbf{TCP/IP}. This allows the \textbf{manager process to reside on any machine in a network} and coordinate the memory usage of all the V8 instances running within the cluster. This makes the framework very \textbf{adaptable} to various use scenarios. For example, if the framework is to be used on a single computer it becomes a special case of the general model: a cluster with one node where the manager and JavaScript applications run on the same machine.
\\\\
\hspace*{1em} A \textbf{JSON} protocol is used to represent the requests and responses exchanged by the V8 instances and manager process. JSON is a good format option as it allows easy extension of the protocol schema, by simply adding new keys the the concerned entities.
\begin{lstlisting}[language=json,firstnumber=1]
{
  "global":{"action":"","error":"", ... },
  "TotalIsolates":Integer,
  "isolates":{
    "1":{ "action":"","error":"", "heap":Integer,"throughput":Float,... },
    "2":{...}
    ...
  }
}
\end{lstlisting}
The \textbf{action} field defines the type of request or response. Possible values for this field are shown below:\\\\
\begin{tabular}{  l  l  l  l  }
  Action & Global & Per Isolate & Description \\
\hline
  status & Yes & Yes & Isolate status request packet\\
  update & Yes & Yes & Isolate update response packet\\
  set\_heap\_size & No & Yes & Sets the heap size threshold over which the GC should intensify\\
  set\_max\_heap\_size & No & Yes & Sets the absolute maximum size the isolate can have\\
  terminated & Yes & Yes & V8 notifies the manager that one of its isolates has finished execution\\
\hline
\end{tabular}
\section{V8}
\hspace*{1em} The V8 virtual machine is based on isolated execution environments, each application runs in an isolated environment without the possibility of accessing resources owned by other applications running within the same V8 process. These environments are called \textbf{isolates}. In essence, each isolate is an instance of the V8 virtual machine, having its own \textbf{heap space}, \textbf{garbage collector} and \textbf{compiler}. Only one thread can access an isolate at a time in order for the isolation to be maintained. Figure \ref{v8arch} illustrates the architecture of the engine.

\begin{figure}[!ht]
  \centering
    \includegraphics[width=0.8\textwidth]{V8Internals.png}
  \caption{V8 Internal Architecture.}
    \label{v8arch}
\end{figure}

\newpage
A \textbf{context} represents a single script as an internal V8 object. JavaScript methods, variables and objects are mapped to the internal object's properties (member variables). The context needs to be added to an isolate's heap in order to be executed. Contexts are attached to an isolate at the same initial depth (global scope) in order for them to share global scope. This causes properties from different contexts that bear the same name to be overridden by the version attached last. 
\begin{lstlisting}[style=htmlcssjs]
 <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <script type="text/javascript" src="http://www.gla.ac.uk/1t4/generic/scripts/libs/jquery/jquery-1.7.2.min.js"></script>
        <script type="text/javascript" src="http://www.gla.ac.uk/1t4/generic/scripts/libs/jquery/jquery-ui.min.js"></script>
        <script type="text/javascript" src="http://www.gla.ac.uk/0t4/generic/video/new/jwplayer/jwplayer.js"></script>
        <script type="text/javascript" src="http://www.gla.ac.uk/0t4/generic/scripts/bbq/jquery.ba-bbq.min.js"></script>
        <script type="text/javascript" src="http://www.gla.ac.uk/0t4/generic/scripts/raccoon.js"></script>
        <script type="text/javascript" src="http://www.gla.ac.uk/1t4/generic/scripts/jquery.flexslider.js"></script>
        <script type="text/javascript" src="http://www.gla.ac.uk/1t4/generic/scripts/main.js"></script>
        <script type="text/javascript" src="http://www.gla.ac.uk/1t4/generic/scripts/video.js"></script>
\end{lstlisting}
The listing above shows an extract of the University of Glasgow website. When the page is loaded, each HTML script tag will cause V8 to create a new context, compile the script source and load it inside the context. These will then be loaded in the same isolate in order to share global scope.
\\\\
An isolate's \textbf{heap} is divided into a set of spaces, as presented by Jay Conrod in his article "A tour of V8: Garbage Collection"\cite{v8gctour}:
\begin{itemize}
\item \textbf{New Space} - Most objects are allocated here. This space is small and is designed to be garbage collected very quickly, independent of other spaces. 
\item \textbf{Old Space} - holds most objects that have pointers to other objects or contain raw data. Objects that survive the new space for a certain amount of time are moved here.  
\item \textbf{Code Space} - contains the instructions that comprise the compiled scripts
\item \textbf{Map Space} - Large key, value map structures that model JavaScript objects.
\item \textbf{Large Object Space} - contains objects that are larger than the size limits of the other spaces. These objects are never moved by the garbage collector.
\end{itemize}
Each space is divided into a set of \textbf{pages}. A Page is a contiguous chunk of memory, allocated using operating system calls. Pages are always 1 MB in size and 1 MB aligned, except in large-object-space, where they may be larger\cite{v8gctour}.
\\\\
\textbf{Garbage collection} is comprised of three algorithms:
\begin{itemize}
\item \textbf{Scavenger} - is a copying garbage collection based on Cheney's algorithm\cite{cheney}. It only operates on the new-space and occurs frequently.
\item \textbf{Mark-Sweep} - operates on the every other space except new-space.
\item \textbf{Mark-Compact} - The same algorithm as mark-sweep with the addition that it compacts live objects in order to improve data locality (with the exception of large object space which is never compacted)
\end{itemize}

Section \ref{gcexplained} has explained how these algorithms work individually, but the key to V8's jank avoidance policy is how they are combined. Jank avoidance is the main reason why JavaScript uses more memory than necessary. By allowing the manager framework to intervene, the memory footprint can be considerably reduced. 
\\\\
\hspace*{1em} The new-space is where most objects are allocated (with the exception of large objects and code segments). This space is kept small (up to 16MB) and is collected frequently. The decision behind this practice is based on empirical observations on common JavaScript behaviour. It has been observed that many objects tend to be short lived, therefore fast allocation, strong locality of reference and short collection times (all benefits of copying garbage collection) are key to obtaining high performance.    
\\\\
\hspace*{1em} The old-space contains objects that are likely to have a longer life. This space is also much larger than the new-space (up to 1GB, 64x times larger), which makes collection much slower. Therefore, it is collected infrequently. To further reduce the impact of the costly collection of the old-space, V8 employs \textbf{incremental marking} and \textbf{lazy sweeping}. 
\\\\
\label{lazyv8}
``Incremental marking allows the heap to be marked in a series of \textbf{small pauses}, on other order of \textbf{5-10 milliseconds} each (on mobile). Incremental marking begins when the heap reaches a certain threshold size. After being activated, every time a certain amount of memory is allocated, execution is paused to perform an incremental marking step.''\cite{v8gctour}
\\\\
``Once incremental marking is complete, lazy sweeping begins. All objects have been marked live or dead, and the heap knows exactly how much memory memory could be freed by sweeping. Rather than sweeping all pages at the same time, the garbage collector \textbf{sweeps pages on an as-needed basis} until all pages have been swept.''\cite{v8gctour}
\\\\
\hspace*{1em} The techniques presented above increase the user-perceived performance of the running application by avoiding noticeable rendering pauses caused by long garbage collection intervals. They \textbf{improve the user experience} by segmenting and dispersing the collection process in time as much as possible, but this inevitably \textbf{causes the memory footprint to be high}.

\subsection{Modifications}
\hspace*{1em} For the purpose of this project, the V8 engine was modified in order to communicate and comply with the manager process. This required both changes to existing code and additions of new code. Figure \ref{v8changes} is an overall diagram of the changes and additions. Red blocks represent additions while blue blocks represent modifications.
\begin{figure}[!ht]
  \centering
    \includegraphics[width=0.85\textwidth]{V8Modifications.png}
  \caption{Modified V8 engine diagram.}
    \label{v8changes}
\end{figure}\\
\begin{itemize}
\item \textbf{Heap} modifications allow setting a heap size limit and retrieving information about the memory usage (memory in use, available memory and total allocated heap memory). 
\item \textbf{Isolate} modifications keep track of all the active isolates within the V8 process, measure execution and garbage collection times and calculate a per isolate performance metric. When a new isolate is crated, the tracker (figure \ref{v8changes}) assigns it an numeric ID and adds it to a list of active isolates. When it finishes execution, its assigned ID is marked as free and the isolate is removed from the list.
\item A \textbf{networking} client execution thread has been added to the V8 process for it to communicate with the manager. It is responsible with connecting to the manager, dividing the data received over the network into packets, decoding them, performing the required actions and sending a response to the manager.
\item A \textbf{protocol implementation} was added in order to represent the protocol messages in a structured manner and simplify serialisation and de-serialisation.

\item \textbf{Encoding} routines were added to the protocol implementation. Protocol packets are encoded to JSON which is then encoded to Base64. The latter uses a restricted set of characters to represent the payload which allows separating packets using ASCII characters that are not part of Base64. This allows the JSON payload to contain any symbols in its fields (even the packet separator character) without altering packet boundaries.

\item \textbf{Configuration} is done through a text file placed in the same folder as the V8 binary. This is meant to simplify integrating the modified V8 into an existing project such as NodeJS and Chrome. If command line arguments were used in stead, they would have to be mirrored in both NodeJS and Chrome projects. The configuration file contains the IP address and port of the manager process.
\end{itemize}
\section{Manager Framework}
% High level description of the manager
\hspace*{1em} The main function of the manager process is to control the memory limits of each isolate. To do this, it needs to keep \textbf{track} of active isolates, \textbf{poll} for status updates, \textbf{calculate} new \textbf{memory limits} for every isolate and \textbf{give feedback} to the user on the status of the system. These functions have been divided into three main components: \textbf{Tracking}, \textbf{Management} and \textbf{Plotting}. Figure \ref{managerdetail} is a detailed component diagram of the manager and is intended to be used as a reference when reading the remainder of this chapter.
\begin{figure}[!ht]
  \centering
    \includegraphics[width=0.9\textwidth]{PythonManager.png}
  \caption{Manager architecture.}
    \label{managerdetail}
\end{figure}
\subsection{Tracking} \label{Design:Tracking}
\hspace*{1em} This module is responsible with interacting and keeping track of the active isolates through the coordinated use of its components: \textbf{Instance Server}, \textbf{Communicator} and \textbf{Registry}. From the point of view of the framework, this module offers a means of communication with active isolates.
\\\\
\hspace*{1em} The registry maintains a \textbf{representation of the managed environment}. Isolates are grouped together based on the V8 process they belong to and the machine they run on. The listing below shows the structure of the registry expressed in JSON:
\begin{lstlisting}[language=json,firstnumber=1]
[{
  "id":"127.0.0.1",
  "memoryLimit":400,
  "v8s":[
    {"id":1,"isolates":[
      {"id":1,"heap":40542,"throughput":1.5},
      {"id":2,"heap":150241,"throughput":15.5},
    ...]
  },
    {"id":2,"isolates":[
    {"id":1,"heap":20541,"throughput":0.56},
        {"id":1,"heap":120443,"throughput":5.21},
    ...]
  },
  ...]
}, ... ]
\end{lstlisting}
In the case portrayed above, there are two V8 processes running on a the same machine as the manager process, each having two isolates. Using a hierarchical storage structure for the tracked isolates has the advantages of \textbf{minimal data redundancy}, ease of \textbf{extensibility} and \textbf{high topology fidelity}. The memory limits produced by the manager must not exceed the memory budget of each machine, this requires knowing the provenance of each isolate. Each isolate is associated a local ID by its host V8, this is used to send memory limits and other commands to the correct isolates. In order to send the commands to the right receiver, the active communication channel for each V8 process needs to be stored as well. If a non hierarchical structure such as a list were to be used, the aforementioned aspects would be more difficult to manage: the communication channel would be stored inside each isolate record adding one redundant field (many isolates can have the same host V8 process), mapping isolates to their host V8 processes and to their host machines would add even more redundant fields, determining which isolates belong to the same machine would become less efficient as it would require iterating through the entire list and mapping isolate updates coming from the network to their respective list records would also require a complete list iteration.
\\\\
\hspace*{1em} The \textbf{communicator} represents an \textbf{open channel to one V8} process. It handles encoding, decoding, sending and receiving messages to/from the V8. A communicator can represent at most one V8 instance at any given time.
\\\\
\hspace*{1em} The \textbf{registry server} is responsible for \textbf{servicing new connections} from V8 instances to the manager. Once a new V8 engine connects to the manager, a record is added to the registry. A new communicator is created for the V8 and is added to its registry record. When a V8 finishes execution (or terminates abnormally), its communicator detects the closing of the channel and removes the corresponding V8 record from the registry. Since the communicator is stored in the V8 registry record, this will cause the communicator to cease to exist. 
\subsection{Management}
\hspace*{1em} The management module is responsible for \textbf{controlling the active isolates} by polling for updates, running a policy program that calculates memory limits, sending the calculated limits to the corresponding V8 processes and providing control over the framework to the user. The constituent components of this module are: the \textbf{Policy Executor}, \textbf{Policy Scripts} and a \textbf{Command Line Interface}.
\\\\
\hspace*{1em} The \textbf{Policy Scripts} are a collection of scripts that can be \textbf{loaded} into the manager framework \textbf{at runtime}. Only one such script can be running at any give time. A policy script receives a list of isolates and a memory budget as input and \textbf{outputs the memory limits} for each of the provided isolates. All of the isolates in the input list run on the same machine, the \textbf{memory budget} represents the \textbf{maximum quantity of memory} that JavaScript applications can use on the machine in question. This limitation reflects the fact that isolates running on one machine can not be allocated memory from other machines, therefore the total memory granted to the isolates can not be larger than the amount of physical memory available to the host machine. It follows from this that the policy script needs to be run once for each managed machine running managed V8 instances. 
\\\\
\hspace*{1em} The \textbf{Policy Executor} regularly \textbf{polls} every managed V8 process for status updates. These updates contain information such as: \textbf{in use heap size}, \textbf{total heap size}, \textbf{available memory} and \textbf{performance}, which gets \textbf{copied into} the corresponding isolate record from  \textbf{the registry}. This information is used by the policy script to decide how much memory an isolate should have. The policy executor then groups all of the isolates based on their host machine and runs the current policy script for each machine.
\\\\
\hspace*{1em} The \textbf{command line interface} provides user access to the manager's functionality. Supported operations include changing the used policy script, changing the frequency of the status update polls, getting the latest isolate metrics, setting an isolate's memory limit and changing the behaviour of the plotting facility. A full list available commands can be found in appendix \ref{allcommands}.

\subsection{Plotting}
\hspace*{1em} \textbf{Feedback} is crucial to evaluating the effectiveness a policy. This is why the manager application has a component dedicated to \textbf{gathering every status update} received by the registry and plotting a selected set of values on screen. Each full screen of plot history is saved in a PNG file, while the full history of the isolate's selected metrics is saved to a CSV file for post-execution analysis. This module is comprised of three components: \textbf{Plotter}, \textbf{Plot Server}, \textbf{Plotting Service}.
\\\\
\hspace*{1em}  The \textbf{plotter} has the sole responsibility of receiving plot data, drawing it on screen and saving it to a file. It is modelled as a separate process from the manager framework. Plotters are started on demand, depending on how many isolates are being managed, each one represents a single isolate. 
\\\\
\hspace*{1em}  The \textbf{plotting service} is responsible with mapping a plot stream to one plotter process. A plot stream is comprised of the metrics from a particular isolate. This service is responsible with detecting new streams, starting plotter processes for them and sending metrics to the correct plotter processes.
\\\\
\hspace*{1em} The \textbf{plotting server} has a function very similar to the registry server discussed in section \ref{Design:Tracking}. It starts plotter processes, when requested by the plotting service, and waits for them to connect to it (via TCP/IP) in order to mark them as idle (ready to plot). The plotter processes are kept in a stack until they are needed by the plotting service. When a stream ends, the plotting service returns the plotter process to the server's stack in order to be used for a new stream.
\\\\
\hspace*{1em} The \textbf{approach} of having \textbf{multiple separate processes} handling plotting was chosen over having one process updating multiple windows (or multiple subplots) because the plotting library is not designed to manage multiple independent plot windows from the same process. A single separate process, resposible with all plotting operations, would be more efficient as it would not require the operating system to schedule large number of processes when the managed isolate count is large. 

\section{Software Engineering}
\hspace*{1em} The \textbf{separation of concerns} principle was applied at two levels. At the first level, the responsibilities of the modified V8 and the management framework were divided. Because the V8 engine executes JavaScript, any additional changes could potentially slow it down, not only by having more threads to schedule as part of the same process but because some of the added code would run inside the thread executing garbage collection for each individual isolate, directly impacting its performance. For this reason the design required the V8 modifications to be a simple and fast as possible shifting the more complex analysis, aggregation and memory limit calculation responsibilities to the manager. \textbf{V8 only calculates instantaneous metrics} for its host applications because the \textbf{manager framework} is better equipped (has global information) to decide what \textbf{metric transformations} (mean,sum,multiplication, etc.) would better reveal the state of an isolate. At the second level, a framework component only implements the \textbf{functionality required to fulfil its purpose}.
\\\\
\hspace*{1em} Since components are only concerned with their own functionality, interaction and passing data between them needs to be facilitated. This is achieved by implementing components using classes. Each component that needs to interact with others receives references to the others either in the constructor call or through an accessor method. This way, all of the components are \textbf{instantiated independently} and are then \textbf{``wired'' together} using references. This makes the system \textbf{loosely coupled}, highly \textbf{modular} and facilitates \textbf{extensibility}. For example, if a new plotter service were to be implemented, an instance to the new service class would be passed to the registry component. No code changed unrelated to the new component would need to be carried out. New components would have to implement an interface describing the methods needed to integrate with the framework. In the case of a new plotter service, the \texttt{update()} method would need to have the same signature in order for the registry to successfully pass data to the it. 
\\\\
\hspace*{1em} The \textbf{memory management policy} is the core component of the system, but because of the exploratory nature of this project it is subject to frequent change. For this reason, multiple policies have been developed in the form of separate scripts that can be \textbf{loaded into the framework at run-time}. Each policy needs to implement an interface in order to interact with the rest of the framework. This further \textbf{lowers system coupling} and increases extensibility. A third party could easily develop an additional policy and start using it without having to change any of the framework code.
\newpage
\chapter{Implementation}
\hspace*{1em} This chapter describes the implementation details and decisions for both the manager framework and V8 modifications. 
\section{Overview}
\hspace*{1em} There are three main areas of the implementation
\begin{enumerate}
\item Building the manager framework
\item Changing the V8 engine to allow it to be controlled by the framework
\item Building a V8 wrapper responsible with running JavaScript applications specified through command line arguments.
\end{enumerate}
\hspace*{1em} The source code can be found at \url{https://github.com/iFlex/SociableJavascript}.
\\\\
\hspace*{1em} A V8 isolate, by itself, is an execution environment, the specific script that needs to run has to be \textbf{compiled} and \textbf{loaded} into the isolate. This is done using the functions provided by said isolate. The \textbf{V8 wrapper} is a separate application that imports the V8 API and reads a set of scripts provided as command line arguments. It then uses the V8 API functions to compile the scripts, creates a new isolate and loads the compiled scripts in order to be executed. It also takes a size expressed in megabytes and sets it as the maximum heap size when creating the isolate. \textit{This program is used to run a set of scripts as a standalone process}.
\\\\
\hspace*{1em} The \textbf{Google V8} engine is implemented in \textbf{C++}, amounting to 800kSLOC. Changing it requires using the same language which does not allow much freedom of choice. However, the management framework is an independent process and can be implemented in any language. \textbf{Python} was chosen for this task for the following reasons:
\begin{itemize}
\item It is a high level language with a small code footprint.
\item It is interpreted, which allows loading and executing code at runtime and also eliminates compilation waiting time (the V8 wrapper, despite its small size, has a compilation delay of approximately 30 seconds).
\item Has extensive support from its community with plenty of useful libraries available.
\item Has built-in support for most of the required constructs to build the management framework including popular data representation protocols such as Base64 and JSON.
\end{itemize}
One major \textbf{drawback of Python} is its interpreted nature, which makes it very slow compared to other languages such as Java, C++ or Golang. However, the \textbf{current focus} of the project is to develop a \textbf{proof-of-concept framework}. This favours using a language that simplifies the development process while compromising speed because more attention can be directed towards devising memory management policies and evaluation of results.
\\\\
\hspace*{1em} The changes carried out on the V8 engine involve allowing the heap size limit to be set per-isolate at runtime as well as adding a network client responsible with connecting to the manager framework, applying the limits (or other commands) received from the manager and replying with status updates. This means that the manager can run on a separate node and manage multiple remote V8 instances.
\\\\
\hspace*{1em} JSON was used for the communication protocol employed by the V8 instances and the management framework because it is human readable and supports nested data structures. This is appropriate as the project aims for correctness rather than performance at this stage. Human readability allows for easy debugging while nesting support contributes to a clear representation of the environment topology.
\\\\
\hspace*{1em} The manager framework implementation uses built-in Python functions for JSON and Base64 encoding and decoding while the V8 modifications use custom built Base64 functions and a third party JSON C++ library.
\section{V8 Client}
\hspace*{1em} The main responsibility of V8 is to run scripts as efficiently as possible. Any form of \textbf{computation} not related to executing the scripts would be perceived as \textbf{overhead} by the user. Garbage collection is one example of such additional computation as it is responsible with freeing memory rather than advancing the execution of the hosted scripts. For this reason, collection pauses are kept as short and far apart as possible. The \textbf{changes} added to V8, for the purpose of this project, are not related to script execution, which would classify them as computational overhead. For this reason, the changes were designed to have a \textbf{minimal performance impact}. This was achieved through a set of implementation decisions detailed below.

\subsection{Enforcing Limits}
\hspace*{1em} By design, each V8 isolate is configured with memory limits for its old-space and new-space when the isolate is created. In the current production V8 these limits do not change throughout the isolate lifetime.
\begin{lstlisting}[language=cpp]
Isolate::CreateParams create_params;
create_params.constraints.set_max_old_space_size(max_heap);
create_params.constraints.set_max_semi_space_size(16);

Isolate* isolate = Isolate::New(create_params);
\end{lstlisting}
\hspace*{1em} The memory spaces constituting the heap are divided into \textbf{1 megabyte pages}
\footnote{The V8 pages are high-level structures which are 
entirely distinct from the OS pages, which are 4K in size and map directly onto hardware frames in physical memory}.
The two functions from the listing above take in values representing a number of megabytes. Since this is an extract from the V8 wrapper, the new-space size is set to 16 megabytes, while the old-space is set to the value of the first command line argument. This also determines the level of \textbf{granularity} that the manager framework can use to determine memory limits. According to the creators of V8, the maximum amount of memory an isolate can use is \textbf{1 gigabyte}\footnote{The 1GB limit is due to a 32bit address space and the use of bit-stealing in references}. Any value above that would cause the garbage collector to behave erratically\cite{v8sizebug}. This gives the manager a \textbf{range of 1000} possible memory limit values enforceable by V8.
\\\\
\hspace*{1em} When a new isolate is created, the value of the argument passed to \texttt{set\_max\_old\_space\_size()} method is copied into the \texttt{max\_old\_generation\_size\_} member variable of the heap class. This class models the heap of a single isolate, it holds pointers to all of the spaces comprising the heap and contains member methods that control garbage collection. The heap is allowed to expand up to the limit set at isolate creation. Normally this limit is larger than the initial heap size. However, if the limit is smaller than the current heap size, the garbage collector will strive to free as much memory as possible in order to comply with it. This enables enforcing heap memory limits at run-time by simply updating the  \texttt{max\_old\_generation\_size\_} variable. To do so, a setter method called \texttt{setMaxOldGenerationSize(int)} has been added to the heap class.
\\\\
\hspace*{1em} Another way of enforcing memory limits at run-time would be to alter the jank avoidance techniques presented at Section \ref{lazyv8}. The alterations would force incremental marking to run without interruption while the lazy sweeping would be forced to free memory until either the heap size becomes smaller than the limit or no more memory can be freed. This is not a preferable solution because it introduces longer garbage collection pauses which would be observable to the end-user. This approach also adds additional computation to the garbage collection process when compared to a simple copy operation employed by the setter function mentioned above.
\subsection{Measuring Performance}
\hspace*{1em} An \textbf{important metric} that needs to be calculated and reported back to the manager is the application \textbf{performance}:
\begin{equation}
performance = \frac{\textit{Execution Time}}{\textit{Garbage Collection Time}}
\end{equation}
\hspace*{1em} This reflects how much time is spent executing JavaScript code versus the time spent performing garbage collection. Therefore, a value of 1 means that V8 spends an equal amount of time executing the application and collecting garbage. A value greater than one means more time is spent executing the application while a value lower than 1 means that more time is spent performing garbage collection. The latter implies that the application is somewhat ``struggling'' to execute as more computation is used to free memory. Ideally, more time should be spent executing the application than collecting garbage. This is what V8 normally optimises for by keeping garbage collection pauses small and dispersed. A high performance metric implies reduced jank observed by the user in the browser.
\\\\
\hspace*{1em} \textbf{Performance is calculated} by measuring the length of the \textbf{garbage collection pauses} and the \textbf{intervals between the pauses}. The V8 application programming interface (API) provides callback functionality for the garbage collection start and end on a per-isolate basis. V8 defines the start of the garbage collection as the prologue and the end as the epilogue, hence the callback functions are named \texttt{gcPrologue()} and \texttt{gcEpilogue()}. The two durations, execution time and collection time, are obtained by subtracting the timestamps captured by \texttt{gcPrologue()} and \texttt{gcEpilogue()}. These are stored in isolate member variables called \texttt{timePrologue} and \texttt{timeEpilogue} respectively. 
\begin{equation}
\texttt{ExecutionDuration} = \texttt{timePrologue} - \texttt{timeEpilogue}
\end{equation}
\begin{equation}
\texttt{CollectionDuration} = \texttt{timeEpilogue} - \texttt{timePrologue}
\end{equation}
\textbf{Execution duration} is calculated when \texttt{gcEpilogue()} runs because the value of \texttt{timeEpilogue} was captured when the previous garbage collection pause ended, making it the starting time of the current execution interval. \textbf{Collection duration} is calculated when \texttt{gcEpilogue()} runs because the value of \texttt{timeEpilogue} is updated with the current garbage collection end time, making the difference between the two equivalent to the duration of the collection process. 
\\\\
\hspace*{1em} The above calculations are accurate with the exception of the moment when the first \texttt{gcPrologue()} is called. At this point, \texttt{timeEpilogue} is not initialized, making the value of \texttt{ExecutionDuration} invalid. To mend this, \texttt{timeEpilogue} is \textbf{initialised with the current time-stamp} when the isolate is created. Below is a listing of V8 modifications that calculate the performance measurement:
\begin{lstlisting}[language=cpp]
bool Isolate::Init(Deserializer* des) {
  ...
  // initialise GC callbacks
  timeEpilogue = high_resolution_clock::now();
  this->heap()->AddGCPrologueCallback(staticGCPrologue,GCType::kGCTypeAll,true);
  this->heap()->AddGCEpilogueCallback(staticGCEpilogue,GCType::kGCTypeAll,true);

  return true;
}

void Isolate::gcPrologue(GCType type, GCCallbackFlags flags){
  timePrologue = high_resolution_clock::now();
  auto duration = duration_cast<microseconds>( timePrologue - timeEpilogue ).count();
  executionTimes[gcIndex] = (int) duration;
}

void Isolate::gcEpilogue(GCType type, GCCallbackFlags flags){
  timeEpilogue = high_resolution_clock::now();
  auto duration = duration_cast<microseconds>( timeEpilogue - timePrologue ).count();
  gcTimes[gcIndex] = (int) duration;
  
  if(executionTimes[gcIndex] < 0)
    executionTimes[gcIndex] = gcTimes[gcIndex];
  
  calcPerformance();  
  gcIndex ++;
  gcIndex %= sampleLength;
}

void Isolate::calcPerformance(){
  long long divBy = gcTimes[gcIndex];
  double tp = 100;
  
  if(divBy != 0)
    tp = ((double)executionTimes[gcIndex])/divBy;

  lastPerformance = (tp > 100)?100:tp;
}
\end{lstlisting}
\hspace*{1em} In the above listing, the time-stamps are stored in circular buffers because, initially, the V8 modifications were intended to return average values of the performance. This approach was abandoned because the manager framework is able to poll V8 instances at configurable frequencies, and also because it is better suited to decide what alterations should be performed on the status information in order to better reflect the state of the isolate. This way, V8 is only responsible with returning \textbf{instantaneous status metrics}, which in turn \textbf{minimises} the \textbf{computational overhead} added by the changes. 
\subsection{Compiling Status Information}
\hspace*{1em} The \textbf{manager} is designed to \textbf{poll} all the tracked V8 instances for \textbf{status updates}. For every isolate belonging to a V8 process, the manager will send a status \textbf{update request}. Upon receipt of the request, the host V8 process must \textbf{collect status information} for the specified isolate and send it back to the manager. This operation must not be costly in terms of \textbf{computation} as it would \textbf{impact} the \textbf{performance} of the running scripts. This was achieved by adding \textbf{accessor methods} that sum up a small number of values or simply return one value. Some such functions rely on underlying V8 accessor methods which had to be \textbf{carefully selected} because some traverse the pages comprising the heap in order to return the size while others return the value of a member variable. The methods that traverse the heap were avoided because of the considerable performance cost they incur since the aim of the project is to maintain high performance while adding global allocation resource supervision. Below is a listing of the accessor methods added to the isolate class.
\begin{lstlisting}[language=cpp]
long long Isolate::getHeapSize(){
  return (long long) (heap_.old_space() ->Size() + heap_.map_space() ->Size() +
                      heap_.new_space() ->Size() + heap_.lo_space()  ->Size() +
                      heap_.code_space()->Size()
  );
}

long long Isolate::getMemoryFootprint(){
  return (long long)(heap_.old_space() ->Capacity() + heap_.map_space() ->Capacity() +
                     heap_.new_space() ->Capacity() + heap_.code_space()->Capacity() +
                     heap_.lo_space()  ->Size()
  );
}

long long Isolate::getAvailableHeapSize(){
  return (long long)(heap_.old_space() ->Available() + heap_.map_space() ->Available() +
                     heap_.new_space() ->Available() + heap_.code_space()->Available()
  );
}

double Isolate::getPerformance(){
  return lastPerformance;
}
\end{lstlisting}
\subsection{Isolate Tracking}
\hspace*{1em} \textbf{V8} is meant to be \textbf{embedded in larger projects} that employ its functionality. The host project would decide what scripts to run and use V8 to compile and execute them. The isolates would be created outside of the V8 source code as it is simply an engine offering functionality to its host program. Because of this, there there is \textbf{no functionality} that \textbf{tracks} the number of \textbf{active isolates} in V8. The manager framework needs a way to identify and track the active isolates belonging to each V8 process. This can be done either in the V8 source code or in the program it is embedded in (V8 wrapper in the case of this project). The latter involves much less overall V8 code changes but does makes it very hard to adopt the modified engine in a different project, such as NodeJS and Chrome. This is why isolate tracking was added to the V8 source code.
\\\\
\hspace*{1em} Upon \textbf{creation}, an isolate is assigned a \textbf{unique identifier} and a \textbf{pointer} to the isolate object is stored in an array. This number is used by the management framework to \textbf{address the isolate} it wants to control. The V8 network client (detailed below) uses the identifier of the isolate to retrieve the stored object pointer and call the appropriate member functions in order to carry out the command received from the manager. Upon \textbf{disposal} of the isolate, its \textbf{identifier} becomes \textbf{free} and can be used to address a new isolate and the \textbf{pointer} is \textbf{removed} from the array. Relevant code fragments are shown below.
\begin{lstlisting}[language=cpp]
int ISOLATE_INDEX = 1;
std::map<int,Isolate *> allIsolates;
std::list<int> freeIDs;
pthread_mutex_t count_mutex = PTHREAD_MUTEX_INITIALIZER;
...
Isolate::Isolate(bool enable_serializer){ 
  //constructor call modified to add isolate pointer to map
  ...
  Isolate::addNewIsolate(this);
}
Isolate::~Isolate() {
  //destructor modified to delete pointer to isolate
  Isolate::removeIsolate(this);
  ...
}
void Isolate::addNewIsolate(Isolate * i){
  pthread_mutex_lock(&count_mutex);

  int id = 0;  
  if(freeIDs.empty()){
    id = ISOLATE_INDEX++;
  } else {
    id = freeIDs.front();
    freeIDs.pop_front();
  }

  i->setIsolateId(id);
  allIsolates[id] = i;

  pthread_mutex_unlock(&count_mutex);
}

void Isolate::removeIsolate(Isolate *i){
  pthread_mutex_lock(&count_mutex);
  
  int id = i->getIsolateId();
  if(allIsolates.erase(id))
    freeIDs.push_front(id);
  
  pthread_mutex_unlock(&count_mutex);
}
\end{lstlisting}

\subsection{Client}
\hspace*{1em} A \textbf{TCP/IP client} was added as a separate execution thread to the V8 process. Its purpose is to continuously attempt to \textbf{connect} to the \textbf{manager} framework until a successful connection is established, after which it \textbf{waits for commands}, \textbf{applies} then by calling the appropriate V8 API functions and \textbf{responds} with status updates. A \textbf{separate thread} was used in order to allow the \textbf{isolates} to run \textbf{undisturbed} and unaware of the existence of the client. This way, the status updates reflect the \textbf{instantaneous state} of the isolate at the moment when the command arrived. Before starting the client, a file called \textbf{V8RemoteControlConfig.txt} containing the manager \textbf{IP} address and \textbf{port} is parsed. The provided IP and port numbers are used to connect to the manager. If this file does not exist, the server reverts to the \textbf{localhost} address 127.0.0.1 with port 15004 assuming the manager is located on the local machine. A \textbf{file} was used to configure the destination IP and port rather than \textbf{command line arguments}, because in the future, this extension might be included in applications like NodeJS, NWJS and Chrome which would require adding the \textbf{additional command line options} to each of these three projects' source codes. A modified build of the \textbf{Chrome} web browser was created for the purpose of this project. Having the V8 client configure itself using a file eliminated the need to change any additional Chrome specific code, which made the integration of the modified V8 matter of replacing the modified files and slightly changing the build procedure to include the new files.
\subsection{Communication Protocol}
\hspace*{1em} The last set of changes added to the V8 project implement the \textbf{communication protocol}, including \textbf{encoding} and \textbf{decoding}. The protocol, described at Section \ref{Design:Tracking}, is based on objects called \textbf{actions}. They can be either \textbf{requests} or \textbf{responses} and can refer to either the \textbf{V8 process} itself (global request or response) or to one of its \textbf{isolates} (local request or response). The action object contains the name of the command to be applied (or the kind of response being returned) and a set of fields containing the required information. A request would be comprised of one global action object and an array of local action objects. The number of local action objects corresponds to the number of isolates the V8 is running. Figure \ref{protocoldiag} is a diagram of the classes that model a protocol message.     
\begin{figure}[!ht]
  \centering
    \includegraphics[width=0.80\textwidth]{ProtocolClasses.png}
  \caption{Protocol entities grouped into classes.}
  \label{protocoldiag}
\end{figure}
\\
\hspace*{1em} The \textbf{details class} is comprised of public fields representing every possible piece of information that can be put in an action object along with functions used to build the object from a JSON string and to convert it to such a string. Each Action class is comprised of a \textbf{name field}, used to determine what request or response it represents, a \textbf{details object} for the additional information and an \textbf{error object}. The latter is used to represent any kind of error encountered while performing the action. The \textbf{command class} contains an action object representing the \textbf{global action}, array of action objects representing the \textbf{isolate level actions} and an \textbf{error object}. The latter is used to represent errors at a command level. If the array of actions was longer than the array of isolate object pointers, meaning that the manager sent limits for more isolates than are actually active, would qualify as a local error and will be represented in the last action object of the actions array from the command object. If the message received from the network could not be decoded, the global error object of the command will be filled in with information about the error and the execution of the command will be aborted.
\\\\
\hspace*{1em} A \textbf{third party library} called \textbf{JsonCpp} was added to V8 in order to handle encoding and decoding JSON. The V8 engine does support JSON, since it stands for Java Script Object Notation, but it is implemented in JavaScript. This means that in order to encode or decode JSON, JavaScript code would have to be run in one of the available isolates or in its own one. This would cause framework related code to compete with hosted application code for execution time. The third party library code will run in the separate thread of the network client and therefore would not have a considerable impact on the execution time of the hosted applications. JsonCpp parses JSON strings and produces \texttt{Json::Value} objects. These are then passed to the appropriate instances of the protocol classes in order to load the values into the appropriate member variables. Every class comprising the protocol implementation with the exception of the Command class has a \texttt{void deserialise(Json::Value obj);} method which loads values from a JSON object. Conversely, a \texttt{void serialise(Json::Value \&obj);} method does the opposite operation when the protocol message needs to be encoded in order to be sent to the manager. The Command class is responsible with performing encoding and decoding from and to string, it also has the two aforementioned methods but in the following format: \texttt{std::string serialise();} and \texttt{void deserialise(std::string);}. This makes the decoding of a JSON string received over the network from the manager as simple as calling one function: \texttt{command.deserialise(string);}.
\\\\
\hspace*{1em} Base64 encoding and decoding procedures were added to the V8 project for the same reason the built in methods were not used to parsing JSON. Handling messages from the manager requires reading data from the network in a string until the separator is read, instantiating a Command object, decoding the string received over the network using Base64 and then passing it to \texttt{command.deserialise();}. Similarly, sending a response to the manager requires modifying the command object to reflect the state of the last performed action, calling \texttt{command.serialise()}, base64 encoding the resulting string, appending the separator and sending it over the network.
\section{Manager Framework}
\hspace*{1em} The role of the manager framework is to keep track of active isolates, control their memory usage by applying a management policy, give feedback on the status of each isolate to the user and allow the user to control the framework. There are three main modules comprising the framework: \textbf{Tracking}, \textbf{Management} and \textbf{Plotting}. These are divided further into more specialised scripts (manager source files can be found at \url{https://github.com/iFlex/SociableJavascript/tree/master/Manager}).
\dirtree{%
.1 \textbf{Management}.
.2 \textbf{Communication}.
.3 \text{server.py}.
.3 \text{communicator.py}.
.3 \text{requestBuilder.py}.
.2 \text{monitor.py}.
.2 \text{policy.py}.
.2 \text{cli.py}.
.1 \textbf{Plotter}.
.2 \text{IpcPlotWrapper.py}.
.2 \text{plotter.py}.
.1 \textbf{PlotFacility}.
.2 \text{PlotServer.py}.
.2 \text{PlotService.py}.
.1 \text{main.py}.
}
\hspace*{1em} Tracking and Management share a \textbf{core component}: the \textbf{registry} (implemented in \texttt{monitor.py}). Tracking accepts connections from V8 instances, creates and associates a communicator instance with each newly open connection and updates the registry to reflect the changes. Any message received by a communicator from its associated V8 updates the corresponding registry record. When a connection is closed, its corresponding communicator automatically updates the registry. The management module scans the registry for active isolates and sends them update requests, it also groups isolates by the machine they belong to and sends them heap limit values. Because of the dependence of the management module on the tracking, the latter has been embedded into management as a sub-module named Communication. However, the plotting module has been divided in two: Plotter and PlotFacility in order to reflect the underlying architecture. The external plotter process from figure \ref{managerdetail} was implemented in the Plotter module as a standalone application while the remaining functionality was implemented by the scripts in PlotFacility. 
%more info on how they get wired
\subsection{Tracking}
\hspace*{1em} The \textbf{registry} keeps track of the state of the monitored environment. It uses a tree-based model implemented using a python dictionary:
\dirtree{%
.1 machines.
.2 \text{Machine\_1}.
.3 \text{V8\_1}.
.4 \text{Isolate\_1}.
.4 \text{Isolate\_2}.
.3 \text{V8\_2}.
.4 \text{Isolate\_1}.
.2 \text{Machine\_2}.
.3 \text{V8\_1}.
.4 \text{Isolate\_1}.
}
The dictionary nodes representing each of the types of nodes show above (machine, v8, isolate) have the following structure: 
\begin{lstlisting}[language=python]
machine = {"FreeList":list(),"v8s":dict(),"id":"0", "memoryLimit":self.newMachineMemLimit}
v8      = {"FreeList":list(),"isolates":dict(),"id":0}
isolate = {"id":0,"created":time.time()}
\end{lstlisting}
\hspace*{1em} When a new machine is added to the registry, its IP address is used as a value for the id field. New V8 and Isolate instances are automatically assigned identification numbers which are unique within the context of the parent node. V8 IDs are unique within the context of their host machine, while Isolate ids are unique within the context of their host V8 processes. The IDs are chosen by finding the smallest number not in used by another instance within the same context. When an Isolate instance is removed, its ID is stored in the free list of its parent V8 record. When a V8 instance is removed, its ID is stored in the free list of its host machine.
\\\\
\hspace*{1em} A \textbf{mutex lock} is used to ensure concurrent changes are carried out safely. Operations that are used by external components such as the communicator and policy executor are considered potentially unsafe and therefore the mutex is used to ensure mutation safety. Functions meant to be called internally do not use the mutex in order to optimise execution speed since locking and unlocking mutexes adds considerable overhead.
\\\\
\hspace*{1em} The \textbf{server} runs in a separate thread that has the responsibility of accepting incoming TCP/IP connections, creating a new communicator object for each new connection and adding the machine that has initiated the connection to the registry if it was not added already.
\begin{lstlisting}[language=python]
soc,addr = self.soc.accept()
addr = str(addr[0])
machineId = self.monitor.getMachine(addr);
if machineId == 0:
  machineId = self.monitor.addMachine(addr); 
else:
  machineId = machineId["id"];
communicator(soc,self.monitor,machineId,self.monitor.update);
\end{lstlisting}
\hspace*{1em} \textbf{Communicators} represent a communication channel to a single V8 instance. Hence, they require a reference to the registry in order to add a new V8 record to it. This reference is passed at the constructor call (\texttt{self.monitor} from the above listing). A new communicator creates a separate thread that listens for messages from the remote V8 process. As soon as a message arrives a method from the registry's interface is called in order to directly update the relevant record. A send function is also part of the communicator interface. This allows other components to send messages to the V8 instances managed by the system. Each registry record of a V8 has a reference to its associated communicator in order for other components to be able to contact the V8 process. Getting a V8's communicator requires calling a registry method and providing a machine ID and a V8 ID: \texttt{getV8Comm(machineId,v8Id)}.

\subsection{Management}
\hspace*{1em} The centre piece of the management module is the \textbf{policy executor} (policy.py). This script is responsible for gathering status updates from every managed isolate and applying a selected policy to the groups of isolates belonging to each machine connected to the manager. This is done periodically at a specified frequency. Algorithm exhibit \ref{policyexec} depicts the execution flow of this script.\\
\begin{algorithm}[H]
 acquire\_registry\_mutex\_lock()\\
 \For{\textbf{each} \texttt{machine} in \texttt{registry}}{
  community = []\\
  \For{\textbf{each} \texttt{V8record} in \texttt{machine}}{
    send\_status\_update\_request()\\
    \For{\textbf{each} \texttt{isolate} in \texttt{V8record}}{
      community.append(\texttt{isolate})
    }
  }
  limits = apply\_policy(community,\texttt{machine}.budget)\\
  \If {validate\_limits(limits)}{
    \For{\textbf{each} \texttt{limit} in \texttt{limits}}{
      send\_memory\_limit\_command(\texttt{limit})
    }
  }
 }
 release\_registry\_mutex\_lock()
 \caption{Policy executor thread}
 \label{policyexec}
\end{algorithm}
Functions such as changing the policy script and changing the polling frequency are exposed to the command line interface to give control over the policy executor to the user. Below is an outline of the policy executor methods.
\begin{lstlisting}[language=python]
class Policy:
    def __init__(self,monitor,frequency,cfgfile)
    def ldConfig(self,fileN)
    def changeSamplingFrequency(self,hz)
    def __loadModule(self,filepath)
    def loadPolicy(self,policyName)
    def logPolicyInfo(self,name,msg)
    def validateSuggestions(self,suggestions,maxMachineMemory)
    def run(self)
\end{lstlisting}
\hspace*{1em} Because policy implementations are intended to be modular and interchangeable, in order to allow the system to load policies at run-time, a standard for implementing one has been outlined. A policy script must implement every method below in order to be successfully loaded and used by the framework. 
\begin{itemize}
\item \texttt{init(context)} - called when the policy is applied for the first time to the isolates of a machine.
\item \texttt{calculate(totalAvailableMemory,isolates,context)} - this method computes the memory limits. It is called at the same frequency as the status polling. The input parameters are a memory budget, a list of isolates with status information, a context object for retaining per-machine state and the return value is a list of the memory limits.
\item \texttt{name()} - returns a string containing the name of the policy
\item \texttt{stats()} - if the policy maintains any status information, returns a string with that information.
\end{itemize}
\hspace*{1em} The \textbf{command line interface} (\texttt{cli.py}) allows the user to control the framework through the use of text commands. A total of 31 commands are available with functions ranging from simply listing the connected isolates, changing the polling frequency, listing available policy scripts to changing the currently running policy script, manually sending memory limits to isolates, changing the total memory limits on machines and determining whether the status information is plotted live on the screen or simply saved to a file for post execution analysis. A listing of the full list of commands can be found in Appendix \ref{allcommands}. Below is a listing of the command line showing the connected isolates.
\begin{lstlisting}
[ MACHINE_127.0.0.1 ]  400 MB
{ V8_1 } 1 MB
  0:0:33.0 (1) available:1048576 average:0 maxFootPrint:1048586 avindex:0 footPrint:1048586 throughput:1.0 v8Id:1 heap:1048576 hardHeapLimit:209715200.0 pMark:True 
_____________________________________________

 V8_2   1 MB
  0:0:6.0 (1) available:1048576 average:0 maxFootPrint:1048586 avindex:0 footPrint:1048586 throughput:1.0 v8Id:2 heap:1048576 hardHeapLimit:209715200.0 pMark:True 
_____________________________________________
\end{lstlisting}
\hspace*{1em} The [] brackets denote the current machine and the \{\} brackets denote the current V8 that the command line is ``watching''. This is mean to simplify commands that need to specify a destination machine, V8 and isolate ID by only providing the isolate ID. The command line interface is \textbf{versatile} enough to be used to \textbf{configure the environment}. This is done using text configuration files which contain commands (one per line). These are applied sequentially when the policy executor object is initialised. A configuration file can be specified via a command-line parameter when starting the manager: \textbf{./main.py config=fullFeatures.txt} 

\subsection{Plotting}
\hspace*{1em} The main \textbf{feedback} mechanism for the frameworks is \textbf{plotting metrics} received from isolates. This is done using the python matplotlib library \cite{matplotlib}. The framework creates a plot window for each isolate that is being controlled and sends the isolate status fields to the corresponding windows as soon as they arrive from the host V8 instances. This is done through registry code: as soon as an update comes on the network, after the registry has been updated, the information is also set to the plotting facility.
\\\\
\hspace*{1em} Plotting involves associating each isolate with a plot window, sending the information concerning an isolate to the correct window and plotting it. To handle this, the plotting module has been split in two sub-modules:
\begin{itemize}
\item \textbf{Plot Facility} - responsible with creating new plotter windows when needed, associating isolates with plotter windows and sending status information to the correct window. 
\item \textbf{Plotter} - is a separate process instantiated by the plot facility on demand, responsible with receiving plot data and drawing it on the screen.
\end{itemize}
\hspace*{1em} The plotter also saves the data it receives in two types of files: a full history of the status information stored in a CSV file and a sequence of PNG files. The latter are captured when the plot data fills the entire surface of the window or when the isolate finishes execution.
\\\\
\hspace*{1em} \textbf{Plotting} while the framework controls isolates comes with \textbf{considerable overhead}. Operations such as redrawing the lines on the screen as well as starting new plotter processes (which start plotting windows) are quire costly in terms of computation and therefore need to be handled with care. In order to minimise the overhead brought by such operations a number of \textbf{optimisations} have been implemented:
\begin{itemize}
\item \textbf{Reusing Plot Windows} - when an isolate finishes execution its corresponding plotter window is put in an idle state. Idle windows are stored in a stack by the Plot Facility to be reused when new isolates begin execution. This way some of the overhead of creating new windows is circumvented. 

\item \textbf{Skipping Render Steps} - by default, the policy executor asks for status updates at a frequency of 10Hz. Receiving data at high frequency causes the plot library to do a large amount of computation because it has to redraw the whole graph surface. This can cause it to fail to show an up-to-date view of the isolate's state, because previous plotting operations do not finish before new data arrives. To mitigate this, the plotter is configured with a target time interval between plot updates. It then measures the time taken to draw the updates. If this time is larger than the configured interval then some of the upcoming updates would not be drawn on screen, creating a skip-ahead effect that ensures the plot window does not lag behind.
\begin{equation}
  \textit{NumberOfUpdatesToSkip} = floor\left(\dfrac{\textit{DrawDuration}}{\textit{ConfiguredInterval}}\right)
\end{equation}

\item \textbf{Post Execution Analysis} - Live updating can be costly even with the above optimisations. Depending on the nature of the experiments being carried out, live plotting can be deactivated in favour of saving the full plot history of one isolate in a CSV file. The data in this file can be plotted after execution has finished. This completely eliminates the overhead of redrawing plot lines as well as creating graphical plot windows. 

\item \textbf{Data Aggregation} - The registry is capable of aggregating the metrics of all isolates belonging to a machine into one single data stream. In stead of sending every data stream from every isolate to the plot services component, it only sends one data stream per machine. This greatly reduces the amount of processes needed to plot the metrics, which reduces the computation load on the manager framework. The user can enable this functionality using the following command: ``setPlotMode MACHINE''.
\end{itemize}

\subsection{Data Flow}
\hspace*{1em} Data flows through the system from the Tracker through Management to Plotting. The policy executor will send out status update requests for each tracked V8 through its associated communicator then it will group isolates belonging to the same machine, calculate memory limits for them and send them out through their host V8 communicators. Figure \ref{dataflow} illustrates the flow of data through the framework.
\begin{figure}[!ht]
  \centering
    \includegraphics[width=0.8\textwidth]{DataFlow.png}
  \caption{Flow of data through the framework.}
    \label{dataflow}
\end{figure}
\\\\
\hspace*{1em} The main priority of the manager is to calculate memory limits and send them to the isolates. Both Sending status update requests and calculating memory limits require iterating the entire registry. This is why these two operation are performed periodically by the same thread. Each status update request prompts a V8 instance to respond, the periodicity of the requests ensure a constant flow of updates. 

\hspace*{1em} Updates from the network are handled by the execution thread of the receiving communicator. This thread handles updating the registry and sends the update to the plotting facility if this feature is enabled. Because updates come at a relatively high frequency the amount of computation needed to handle one request needs to be kept relatively small. For this reason, the functionality illustrated in \cref{dataflow} (bottom right) is performed by a separate thread. The communicator thread simply adds the new update to a queue to be processes by the plot facility thread. This ensures the communicator carries out 2 brief operations per received message: updating the registry and the plotting service. Safe operation is ensured by mutex locks implemented by the registry and the plot facility.
\\\\
\hspace*{1em} The manager framework is mean to run for prolonged amounts of time and process high volumes of data. Debugging the application becomes difficult if all error messages are passed to process standard output. \textbf{Logging} has been added to the system in order to keep a record of the framework status and errors encountered. Each module discussed in this chapter should maintain a log of its own, but due to time constraints logging has been added only to the policy executor and plot facility modules. The policy executor log is key to debugging policy scripts as it stores the errors and warnings for the current policy script. Third party developers could create new such scripts and use the log to check if they exhibit correct behaviour.

\section{Testing}
\hspace*{1em} Testing was facilitated by a set of custom module tests. These were grouped based on the module they test into separate scripts that run them and display results. If any errors are encountered, this would indicate there is a flaw in the scrutinized component. Trace-backs indicate where the problem originates, aiding the solving process. 
\\\\
\hspace*{1em} The registry is a critical part of the system. It was tested using the \texttt{testRegistry.py} script which automatically creates an empty registry adds and removes machines, v8 instances and isolates to it checking if the registry methods operate correctly. A set of isolates are meant to remain in the registry after the tests are complete, they are tagged in order to check if the accessor methods retrieve the correct isolates. The request builder is a component that creates dictionaries representing manager requests and requires access to the registry. It is also tested by this script. 
\\\\
\hspace*{1em} The next important aspect that requires testing is message processing. The script called \\ \texttt{testPacketExchange.py} connects to a V8 instance and tests how well the communicator component parses messages. Packets are exchanged continuously for a certain amount of time, if any parsing errors arise this would indicate that the communicator does not parse the messages correctly.
\\\\
\hspace*{1em} Policies are the most important aspect of the framework. Since they are loaded at run-time, there needs to be a way to test them before they start being used. The \texttt{simulatePolicy.py} script allows a developer to load a policy and test it in two modes.
\begin{enumerate}
\item \textbf{Manual} - \textit{is a step by stem iteration over a small number of input data sets defined my the tester}. At star-up, this mode requires a number of iterations, a number of isolates to test with and the metrics that change at each iteration. The tester needs to manually input values for the metrics that have been declared to change and the output will be printed on screen.
\item \textbf{Automatic} - \textit{is a simulation that applies the policy to a pre-recorded set of isolate metrics}. Testing requires simply specifying a path to a folder containing recorded isolate metrics in CSV format (the output of the plotting facility). This way the simulation will automatically create as many isolates as there are CSV files. At each iteration (each line in the files) isolates are loaded with metrics from the files and passed to the policy. The output can either be viewed on screen or saved to a file. 
\end{enumerate} 
\hspace*{1em} After the critical components have been tested, the final step was to ensure that the manager framework worked as a whole and did indeed affect the amount of memory an isolate could utilise. To do this, the framework was started along with a V8 isolate running the binary tree benchmark. At different intervals of time memory limits were set, using the command line interface, and the memory utilisation was observed. The V8 instance had successfully complied with the limits.

\chapter{Evaluation}
\hspace*{1em} This chapter presents specific memory management policies developed for the purpose of this project and analyses their effectiveness in terms of application survival rate. In order to measure this rate, benchmark scripts have been grouped in test batches with a known total minimum heap size. Each batch is executed with each policy to explore the space of dynamic behaviour.
\section{Memory Management Policies}
\hspace*{1em} Each policy models an \textbf{isolate} as an individual who is \textbf{part of a community}. This community has a certain amount of wealth available. Each individual owns a portion of the total community wealth and has a welfare factor. This factor reflects the well-being of the individual. The aim of the manager framework is to \textbf{maximise the welfare of the whole community by redistributing wealth}. Individuals who have more wealth than they need will have to give a portion of this resource to ones that do not have enough. In technical terms, wealth is equivalent to heap size and welfare quantifies whether an application is struggling to execute or is performing optimally.
\\\\
\hspace*{1em} For each application, the welfare coefficient is calculated based on the metrics received at the status updates. Since there are multiple metrics received, this coefficient can be calculated in a multitude of ways. Each policy calculates welfare in a different manner. A single machine represents a community since its memory resources are not transferable to other machines in a cluster. Therefore, a policy is applied to every community in the tracked environment.
\\\\
\hspace*{1em} For any given isolate, the policy script receives all its \textbf{measured metrics}, which are then \textbf{combined} in order to calculate the \textbf{welfare coefficient}. This is then used to determine the welfare of the whole community and to decide how to redistribute memory. The reason for making all the metrics available comes from the exploratory nature of this project. The performance metric (execution time to garbage collection time ratio) might seem sufficient to determine if an isolate is thriving or struggling, but certain scripts thrive while having a low value for this metric. This implies the need for careful experimentation with various metric combinations in order to devise a good welfare function.
\\\\
\hspace*{1em} Each policy allocates the entire memory budget to the managed applications aiming to maximise the community welfare and the number of programs that can run in parallel. On the one hand, when the memory need is much smaller than the budget, having more memory than needed allows the V8 engine to shorten and disperse the garbage collection pauses in order to improve execution time. On the other hand, when the memory need is near the budget, the set of applications is prevented from extending beyond it, hence avoiding paging and preventing them from keeping unused allocated memory.
\section{Evaluation Scenarios}
\hspace*{1em} In order to evaluate the effectiveness of the memory management policies, a set of JavaScript benchmarks were used (full list at Appendix \ref{benchmarks}). Each script has a minimum heap size, the amount of memory allocated to each script needs to be above this size in order for it to execute successfully. The minimum heap size of each script was calculated using a binary search approach. The \textbf{benchmarks} were grouped together in \textbf{test scenarios}, which execute multiple scripts concurrently. Knowing the minimum heap size of each script facilitates calculating the minimum amount of memory needed to successfully run the scenario. 
\\\\
\hspace*{1em} Knowing the minimum heap size of a test script also provides insight into the impact of limiting a script's memory utilisation. Some scripts are computation bound and use a constant amount of memory. This means that regardless of how much more memory they have available in addition to their minimum heap size, the performance of such scripts will be constant. Other scripts exhibit behaviour modelled by the economic utility function (based on application throughput) introduced by the ``Judgement of Forseti'' research paper:``Utility functions are usually increasing (more commodity gives higher utility), but with diminishing returns (the more the consumer has already, the less benefit they get from an additional unit)''\cite{diminishreturns}. Figure \ref{diminishret} from Section \ref{forsetitalk} depicts the \textbf{diminishing returns effect} by showing the relationship between heap size and throughput.  
\\\\
\hspace*{1em} In the case of this project, \textbf{economic utility} is represented by the \textbf{performance metric}, which in essence reflects how fast the application executes (the less garbage collection, the higher the ratio which implies shorter execution time). The closer the memory limit is to the minimum heap size, the slower it will execute, but because of the diminishing returns effect, increasing the available memory causes execution speed to increase at a slow rate. This gives the management framework leeway to balance execution speeds of managed scripts with memory efficiency. For this reasons all of the various \textbf{benchmark scripts need to be combined} in order to cover as many \textbf{realistic scenarios} as possible. The scripts have been divided into the following categories:
\begin{enumerate}
\item \textbf{Constant} memory utilisation - these scripts have the same performance regardless of how much more memory is available in addition to their minimum heap size. An example for this category is the V8 \textbf{Regexp} benchmark which executes regular expression operations from 50 of the most popular web pages.
\item \textbf{Variable} memory utilisation - these scripts exhibit the above mentioned diminishing returns effect. The more memory is available, the higher the execution speed. An example for this category is the \textbf{Binary Tree} benchmark which allocates and deallocate many many binary trees.
\end{enumerate}
Using these categories, scripts have been combined in the following types of test scenarios:
\begin{itemize}
\item \textbf{All Variable Same Application} (AVSA) - multiple instances of the same benchmark which exhibits diminishing returns with the increase of the heap size.
\item \textbf{All Constant Same Application} (ACSA) - multiple instances of the same benchmark which has constant memory use.
\item \textbf{All Variable Mixed} (AVM) - A mix of benchmarks with different minimum heap sizes, all exhibiting diminishing return with the increase of heap size. 
\item \textbf{All Constant Mixed} (ACM) - A mix of benchmarks with different minimum heap sizes, all having constant memory use.
\item \textbf{Mixed} (M) - A mix of all available benchmarks
\end{itemize}
An additional criteria applied to each type of test scenario is the minimum heap size of its comprising scripts:
\begin{enumerate}
\item \textbf{Large minimum heap} - applications with minimum heap sizes above 50MB
\item \textbf{Small minimum heap} - minimum heap size under 50MB
\end{enumerate}
Using the last two sets of criteria, test scenarios have been created in order to provide a fair mix of scripts. For each of the \textbf{5 main types of scenarios} (AVSA,ACSA,AVM,ACM,M) three scenarios have been created: one with large minimum heap sizes, one with small ones and one with a mix of both large and small, with certain exceptions where a mix of large and small was not possible.
\begin{itemize}
\item \textbf{All Variable Same Application} (AVSA) - 2 scenarios: large, small
\item \textbf{All Constant Same Application} (ACSA) - 2 scenarios: large, small
\item \textbf{All Variable Mixed} (AVM) - 3 scenarios: large, mix, small 
\item \textbf{All Constant Mixed} (ACM) - 2 scenarios: large, small
\item \textbf{Mixed} (M) - 3 scenarios: large, mix, small
\end{itemize}
\hspace*{1em} A scenario is described by a JSON file which lists what scripts to be instantiated and how many times to instantiate them. An example scenario file and further details on ones used for this evaluation are presented in Appendix \ref{scenarioapx}. These files are parsed and applied by a script called \texttt{runscen.py}. After the scenario finishes execution: isolate history (CSV files), individual program standard output and standard error are collected and stored for post-execution inspection. This facility can be called from the manager command line interface as well.

\section{Evaluation Technique}

\hspace*{1em} \textbf{Each scenario} combines scripts that sum up a total \textbf{minimum heap size of 400MB}. In order to measure policy effectiveness, multiple runs of all scenarios have been carried out for each policy. The percentage of surviving out of total run scripts for each scenario reflects the effectiveness of the policy. This percentage is important because the system is not intended to have any prior knowledge about the the minimum heap size of the scripts it manages. It is intended to rely exclusively on information collected from the running isolates at runtime to determine the state of each script and decide how to divide memory. This is mainly because, in a realistic scenario, applications would change their behaviour depending on external, unpredictable, factors such as user input or network events, which makes calculating the minimum heap size impractical. 
\\\\
\hspace*{1em} Constituent \textbf{scripts} of a test scenario \textbf{run on the same machine}, making them members of the same community. The manager framework has been configured with a community (machine) memory budget in order to \textbf{simulate a memory constrained environment}. This kind of environment would better reflect the effectiveness of the policies since it is much harder to redistribute memory without knowing the minimum heap size of each script when the memory budget is only slightly larger than the total minimum heap size.
\\\\
\hspace*{1em} The following  memory budget values have been used for each test run. The slight increase in the budget at each run is meant to relax the memory constraint by a small amount in order to compensate for the policy's lack of knowledge about individual heap sizes. 

\begin{center}
\begin{tabular}{  | l | l | l | }
\hline  
  Run & Memory in addition to minimum heap size & Machine Memory Limit \\
\hline
  1 & 0\% & 400MB \\
\hline
  2 & 5\% & 420MB \\
\hline  
  3 & 15\% & 460MB \\
\hline  
  4 & 35\% & 540MB \\
\hline  
  5 & 50\% & 600MB \\
\hline  
  6 & 100\% & 800MB \\
\hline
  7 & 110\% & 840MB \\
\hline
\end{tabular}
\end{center}
\hspace*{1em} An \textbf{execution time analysis} was performed for each scenario in order to evaluate the effectiveness of the framework itself. Limiting the heap of a script will cause it to finish executing at a later time than if memory were not constrained. Scripts that exhibit the diminishing returns behaviour will be susceptible to execution time increases. If this increase in execution time becomes too large, the framework would be infeasible. The total execution time of each scenario as well as the execution time of each constituent script is measured in order to be compared with execution times where no policy is applied.

\section{Equal Share Policy}
\hspace*{1em} This policy was developed mainly for comparison reasons. The \textbf{approach} it takes is the \textbf{most naïve}: it gives each isolate an equal share of the memory budget. If new isolates connect, the share becomes smaller. If isolates finish execution or disconnect due to failure, the share becomes larger.
\begin{equation}
\textit{HardHeapLimit}_i = \dfrac{\textit{MachineMemoryBudget}}{\textit{NumberOfIsolates}}
\end{equation}
The equation above defines how the memory limit is calculated for each isolate.
\\\\
\hspace*{1em} This policy is \textbf{optimal if} the managed environment is running the \textbf{same application in every community}. This is because every application has exactly the same minimum heap. In order to fit these applications in a certain amount of memory, the best possible approach is to divide the memory equally as long as the share each receives is larger or equal to the minimum heap size. In any other case, \textbf{this policy would not adapt to} the \textbf{different needs} of the applications because it would require a memory budget equal to: \textit{the largest minimum heap size times the number of managed scripts}. In order for this policy to achieve a 100\% survival rate for all test scenarios of this evaluation, it would require a budget of 5GB, which is 12.5 times larger than the minimum heap size required for any of the scenarios (400MB).
\\\\
\hspace*{1em} Figures \ref{equalshare_eq} and \ref{equalshare_df} show Equal Share controlling two concurrent applications. The red line represents the memory limit, the green line represents the heap size and the blue line represents memory in use.

\begin{figure}[!ht]
\subfigure[196MB minimum heap size]{\includegraphics[width=0.5\textwidth]{e_e_left}}
\subfigure[196MB minimum heap size]{\includegraphics[width=0.5\textwidth]{e_e_right}}
\caption{2 concurrent instances of the same application}
\label{equalshare_eq}
\end{figure}
\begin{figure}[!ht]
\subfigure[196MB minimum heap size]{\includegraphics[width=0.5\textwidth]{d_e_left}}
\subfigure[14MB minimum heap size]{\includegraphics[width=0.5\textwidth]{d_e_right}}
\caption{2 concurrent instances of different applications}
\label{equalshare_df}
\end{figure}
In figure \ref{equalshare_eq} the two applications have the same memory need (196MB min. heap) and therefore equal share performs optimally. However, in figure \ref{equalshare_df} the two applications have different memory needs. Splitting the budget in half (115MB) does not not allocate enough memory to the application needing 196MB, causing it to fail.
\section{Robin Hood Policy}
\hspace*{1em} This policy redistributes memory from the wealthiest isolate to the poorest isolate. \textbf{Wealth is equal to the performance metric} of each isolate. Isolates are sorted by their wealth coefficient and a certain amount of memory is taken from the wealthiest and given to the poorest.
\begin{equation}
\textit{StealBudget} = \dfrac{HardHeapLimit_i}{2}
\end{equation}
\hspace*{1em} The amount of memory that can be stolen from the wealthy isolate is half of its memory allowance. This is meant to prevent poor isolates from causing wealthy isolate to suddenly starve for memory and potentially fail by loosing all of their memory.
\begin{equation}
\textit{StealAmount} = StealBudget \times \dfrac{log_2\left(t\right)}{log_2\left(0.001\right)}
\end{equation}
\hspace*{1em} Where \textbf{t} is the execution to garbage collection time ratio of the poor isolate. Constants used throughout this policy are based on empirical observations. The fraction divides the logarithm of t by the logarithm of the lowest observed t (0.001) in order to calculate a percentage representing how much memory the poor isolate needs. A \textbf{t} value of 1 indicates that the isolate is doing an equal amount of execution and garbage collection while a value of 0.001 indicates that is is very likely to fail at the next garbage collection. This way the lower the t value of the isolate, the more memory it will require from the steal budget.
\\\\
\hspace*{1em} The steal amount is then subtracted from the memory allowance of the wealthy isolate and added to the poor one. By default, this happens ten times per second. After one application of the policy, the t values of the isolates are very likely to change, hence different isolates would exchange memory at the next run unless the poor or wealthy isolate are still poor or wealthy respectively. In theory, this policy adapts well to applications with different needs because eventually every ``struggling'' application would be at the end of the sorted array and will receive memory from the wealthiest application. In reality, the fact that only one application is given memory at every policy run leaves other applications unattended for long periods of time, which would cause them to fail before they get a chance to received more memory.
\\\\
\hspace*{1em} Figures \ref{robin_eq} and \ref{robin_df} show Robin Hood controlling two concurrent applications. The red line represents the memory limit, the green line represents the heap size and the blue line represents memory in use.
\begin{figure}[!ht]
\subfigure[196MB minimum heap size]{\includegraphics[width=0.5\textwidth]{e_r_left}}
\subfigure[196MB minimum heap size]{\includegraphics[width=0.5\textwidth]{e_r_right}}
\caption{2 concurrent instances of the same application}
\label{robin_eq}
\end{figure}
\begin{figure}[!ht]
\subfigure[196MB minimum heap size]{\includegraphics[width=0.5\textwidth]{d_r_left}}
\subfigure[14MB minimum heap size]{\includegraphics[width=0.5\textwidth]{d_r_right}}
\caption{2 concurrent instances of different applications}
\label{robin_df}
\end{figure}
\\\\
\hspace*{1em} In figure \ref{robin_eq} the policy manages to keep both applications alive until too much memory is ``stolen'' from one of the applications, causing it to crash. This happens in a scenario where both applications have the same memory need because Robin Hood relies solely on the performance metric, which has been found to reflect welfare poorly due to its large fluctuations. However, in figure \ref{robin_df} the policy exhibits expected behaviour. It steals from the ``rich'' application and give to the ``poor''. This can be observed in the \textbf{symmetry of the red line}, the shape from left graph is a horizontal flip of the one from the right graph. The repeating peaks in the right graph are a result of the policy giving one application memory until it becomes ``richer'' than the other, at which point roles are reversed and it starts taking memory away. 

\section{Inverse Throughput Policy}
\hspace*{1em} This policy considers \textbf{isolate welfare} to be \textbf{equal to the performance metric}, every other metric is not taken into account. In order to calculate how much memory each isolate is entitled to, the inverse welfare of the community is calculated using the equation below:  
\begin{equation}
\textit{CommunityInverseWelfare} = \sum_{i=0}^{N} -\log_2\left(\dfrac{t_i}{1000}\right)
\end{equation}
\textbf{N} is the total number of isolates, \textbf{t} is the performance metric of an isolate. The range of t is from 0 to 100, dividing this number by 1000 ensures that every possible value of t/1000 will have a logarithm with the same sign. The same effect can be achieved by dividing by 101, but it has been heuristically determined that using t/101 tends to generate disproportionately small memory limits for isolates with high t values compared to the ones with small ones. The equation below describes how the memory limit is calculated for each isolate:
\begin{equation}
\textit{HardHeapLimit}_i = \textit{MachineMemoryBudget} \times \dfrac{-\log_2\left(\dfrac{t_i}{1000}\right)}{\textit{CommunityInverseWelfare}}
\end{equation}
\hspace*{1em} The fraction in this equation is a percentage representing how much of the community inverse welfare an isolate is responsible for. This is then multiplied by the memory budget, which equates to how much of the total memory this isolate is entitled to.
\\\\
\hspace*{1em} Figures \ref{invtp_eq} and \ref{invtp_df} show Inverse Throughput controlling two concurrent applications. The red line represents the memory limit, the green line represents the heap size and the blue line represents memory in use.
\begin{figure}[!ht]
\subfigure[196MB minimum heap size]{\includegraphics[width=0.5\textwidth]{e_i_left}}
\subfigure[196MB minimum heap size]{\includegraphics[width=0.5\textwidth]{e_i_right}}
\caption{2 concurrent instances of the same application}
\label{invtp_eq}
\end{figure}
\begin{figure}[!ht]
\subfigure[196MB minimum heap size]{\includegraphics[width=0.5\textwidth]{d_i_left}}
\subfigure[14MB minimum heap size]{\includegraphics[width=0.5\textwidth]{d_i_right}}
\caption{2 concurrent instances of different applications}
\label{invtp_df}
\end{figure}
\\\\
\hspace*{1em} In figure \ref{invtp_eq}, even though the two scripts successfully execute, the limits set by the policy fluctuate frequently from one extreme (too much memory) to the other (too little memory).
\\\\
\hspace*{1em} In figure \ref{invtp_df} the policy causes one application to fail because of its sole reliance on the performance metric to determine isolate welfare. This metric appears to be very unstable, which in turn makes this policy \textbf{unstable}.

\section{Wealth Redistribution}
\hspace*{1em} This policy computes isolate welfare based on a \textbf{weighted sum} of a set of metrics. Using more than one metric is meant to give \textbf{more insight into the behaviour of the isolate}. The used metrics are:
Maximum Heap Size (L), Current Heap Size (H), in use heap size (h), Execution to Garbage Collection ratio (t). For each isolate, the welfare index is calculated as follows:
\begin{equation}
\textit{WelfareIndex}_i = 0.75\times \dfrac{max\left(0,L_i - H_i\right)}{L_i}+0.20\times \left(1 - \dfrac{H_i}{\max_{0 \leq i \leq N}(H_i)}\right)+0.05\times \dfrac{t_i}{100}
\end{equation}
The above equation composes the welfare index of an isolate as 75\% headroom\footnote{Headroom is equivalent to: memory allowance - heap size } to heap size limit ratio, 5\% performance metric and 20\% the inverse\footnote{To be understood as 1 - ratio rather than 1/ratio} of the isolate heap size to maximum observed heap size ratio. 
\\\\
The \textbf{give potential} of an isolate represents the maximum amount of memory it can offer for redistribution. This is equal to the headroom plus the amount of memory allocated to an isolate that is not in use (H - h):
\begin{equation}
\textit{GivePotential}_i = max(0,L_1 - h_i)
\end{equation}
The \textbf{need potential} of an isolate is a percentage representing how much it struggles to execute, or how ``poor" it is:
\begin{equation}
\textit{NeedPotential}_i = 1 - \textit{WelfareIndex}_i
\end{equation}
A \textbf{total give potential} and a \textbf{total need potential} are calculated for the entire community. The total give potential is the total amount of memory that can be redistributed at this iteration.
\begin{equation}
\textit{TotalGivePotential} = \sum_{i=0}^{N}\textit{GivePotential}_i
\end{equation}
\begin{equation}
\textit{TotalNeedPotential} = \sum_{i=0}^{N}\textit{NeedPotential}_i
\end{equation}
\hspace*{1em} The \textbf{Gini index} ``is a measure of statistical dispersion intended to represent the income distribution of a nation's residents, and is the most commonly used measure of inequality''\cite{gini}. This function returns a number in the range 0 - 1 which is equivalent to a \textbf{percentage of inequality}. If one member of a community owned all of the available wealth while the others owned nothing, the Gini index would be 1 (100\% inequality) while if everyone owned an equal share of the total available wealth, the Gini index would be 0 (0\% inequality). This function is used to determine how much of the memory budget needs to be redistributed:
\begin{equation}
\textit{Redistribute} = min(TotalGivePotential,\textit{MachineMemoryBudget} \times \textit{GINI}(\textit{WelfareIndex}))
\end{equation}
\hspace*{1em} The \textbf{Gini function is applied to the welfare indexes} of the isolates and the returned percentage is multiplied by the memory budget in order to determine how much of it needs to be redistributed. If the result is larger than the total redistributable memory calculated before, then it gets set to the maximum redistributable memory.
\\\\
\hspace*{1em} The limits set for the isolates do not always sum up to the total available memory of the community they belong to. This is because isolates complete execution at different times. When this happens, their memory becomes available for redistribution to the others that are still running. This creates an amount of available memory that is not allocated to any application and needs to be redistributed.
\begin{equation}
\textit{Available} = max(0,\textit{MachineMemoryBudget} - \sum_{i=0}^{N}L_i)
\end{equation}
\hspace*{1em} The last step before memory can be redistributed among isolates is to calculate the total amount that needs to be taken from the isolates. Intuitively, this should be equal to the previously calculated redistribution amount (using the gini index), and this is true when the sum of the memory limits equals the total community budget. However, when there is a certain amount of unallocated memory (available memory), less memory needs to be taken away from isolates because the available memory can be used to compensate:  
\begin{equation}
\textit{Take} = max(0,\textit{Redistribute} - \textit{Available})
\end{equation}
\hspace*{1em} Finally, a new memory limit is calculated for each isolate using the following equation:
\begin{equation}
L_i = L_i - \dfrac{\textit{GivePotential}_i}{\textit{TotalGivePotential}} \times Take + \dfrac{\textit{NeedPotential}_i}{\textit{TotalNeedPotential}}\times \left(Available + Take\right) 
\end{equation}
\hspace*{1em} A certain amount of memory is taken form each isolate and a different amount is given back. These two amounts are calculated depending on how much the individual isolate can give compared to the total give potential of the community, and respectively on how much it needs to receive compared to the total need of the community.
\\\\
\hspace*{1em} Figures \ref{wrd_eq} and \ref{wrd_df} show Wealth Redistribution controlling two concurrent applications. The red line represents the memory limit, the green line represents the heap size and the blue line represents memory in use.
\begin{figure}[!ht]
\subfigure[196MB minimum heap size]{\includegraphics[width=0.5\textwidth]{e_w_left}}
\subfigure[196MB minimum heap size]{\includegraphics[width=0.5\textwidth]{e_w_right}}
\caption{2 concurrent instances of the same application}
\label{wrd_eq}
\end{figure}
\begin{figure}[!ht]
\subfigure[196MB minimum heap size]{\includegraphics[width=0.5\textwidth]{d_w_left}}
\subfigure[14MB minimum heap size]{\includegraphics[width=0.5\textwidth]{d_w_right}}
\caption{2 concurrent instances of different applications}
\label{wrd_df}
\end{figure}
\\\\
\hspace*{1em} In both cases, this policy manages to successfully infer the real memory needs of each application and distributes the memory so that both can fit in a 230MB budget. The budget has 20MB added to the minimum heap size to allow the policy some leeway since it has no knowledge of the minimum heap sizes of the scripts it manages (the same budget was used for the other policies).
\section{Pascal}
\hspace*{1em} This is an experimental policy that uses an additional feature of the management framework called \textbf{soft heap limits} in order to put pressure on managed isolates, hence its name. The other policies use \textbf{hard heap limits} which are strictly enforced by the V8 engine. If the heap of a script overflows the hard heap limit and the garbage collector can not free any more memory, the script will fail. In the case of soft heap limits, if the heap overflows the limit, garbage collection will be intensified but the script will not fail execution if more memory can not be freed.
\\\\
\hspace*{1em} The Pascal policy sets the soft heap limit of each script to 0, hence the garbage collection is permanently intensified. This will not reduce memory as drastically as a hard heap limit would, but reduction is still considerable when compared with unrestricted heap utilisation. The main consequence is that the execution time for scripts running with this policy is increased. Figure \ref{pascal} is a comparison between a benchmark script with no limits enforced and the same script run with this policy.

\begin{figure}[!ht]
\centering
\subfigure[No Memory Restrictions]{\includegraphics[width=0.2\textwidth,height=3.4cm]{binarytree}}
\subfigure[Pascal Policy]{\includegraphics[width=0.6\textwidth]{pascal}}
\caption{No Restrictions vs. Pascal policy}
\label{pascal}
\end{figure}

\hspace*{1em} Without any restrictions, the memory usage peaks at approximately 700MB while the pascal managed run peaks at approximately 350MB. The \textbf{memory utilisation is halved} but the execution time is increased. Unrestricted, the run takes approximately 30 seconds to complete while the pascal run takes 1 minute and 30 seconds (3 times slower). For comparison reasons, running the same script with a hard heap limit set to the minimum heap size takes 1 minute (2 times slower). The version that uses soft heap limits is slower because it forces the garbage collection to happen more often (even when it is not needed or memory can not be freed) than the version using hard heap limits.
\\\\
\hspace*{1em} Even though this policy causes the environment to run slower than both an unrestricted run or one that is controlled using hard heap limits, it is a \textbf{good candidate for comparison} as it \textbf{guarantees that all managed scripts will execute successfully}.

\section{Findings}
\hspace*{1em} Every test scenario was run against each policy in order to measure the isolate \textbf{survival rate} (number of survived isolates vs. total number of isolates). At each run the memory budget was slightly increased in order to ease the process of allocating memory. The initial goal was to observe the growth of survival rates as the memory budget increased. Figure \ref{findings} shows the graph of the survival rates for the 4 policies at each run, the y-axis represents survival percentage while the x-axis represents the memory budget set for the run (value in brackets represents how much larger the budget is compared to the total minimum heap size).
\begin{figure}[!ht]
  \centering
    \includegraphics[width=0.7\textwidth]{SurvivalRates.png}
    \caption{Survival Rates of each policy}
    \label{findings}
\end{figure}\\
\hspace*{1em} Robin Hood experiences \textbf{fluctuations in its survival rate}, most likely due to the fact that it only redistributes memory between two isolates at every iteration. This causes the isolates in between the richest and poorest to remain unattended for ,potentially, long  periods of time, which would lead to their failure. Its unpredictability makes it \textbf{unsuitable for use in a real system}.
\\\\
\hspace*{1em} Inverse Throughput experiences growth as the memory budget is increased, but only reaches a maximum of 99\% survival rate. The fact that the maximum is reached at run 6 and maintained at iteration 7 strongly indicates that any further increases in memory budget will result in diminishing returns. However, the only way to know for certain is through further testing, which makes this policy \textbf{infeasible because of the unknown} (potentially large) \textbf{budget threshold} needed to achieve complete survival.
\\\\
\hspace*{1em} Equal Share, surprisingly, experiences a similar growth in survival rate as Inverse Throughput, reaching a maximum of 98\% at run 6 and maintaining it in run 7. However, in the case of this policy, the \textbf{memory budget needed} to achieve 100\% can be calculated and is equal to \textbf{5GB}. It is likely that scripts with large minimum heap sizes fail and their memory is redistributed to the others. Since there are far more scripts with small minimum heap sizes, the survival rate is high. This behaviour is due to the policy's lack of consideration for runtime metrics, with makes it \textbf{infeasible}.
\\\\
\hspace*{1em} Wealth Redistribution is the only policy to successfully run the scenarios with no failed scripts. It exhibits the expected growth in survival rate, starting at 95\% when memory is most constrained and achieving 100\% when the memory budget is equal to double the minimum required heap size (800MB).
\\\\
\hspace*{1em} A memory budget need of double the minimum heap size for successful memory management might seem considerable. However, this needs to be put into perspective in order to effectively determine whether this can be considered a success. To do so, the 12 scenarios were run without any restrictions in order to measure the maximum memory utilisation they achieve as well as an average of the maximum utilisations of each scenario and an overall average utilisation. Even though the \textbf{minimum heap size} required for every scenario was \textbf{400MB}, the \textbf{maximum memory utilisation was 3GB (7.5x minimum heap size)} while the \textbf{average maximum memory utilisation for all scenarios was 1,2GB (3x minimum heap size)}. A detailed comparison of memory utilisation for each scenario can be found in Appendix \ref{memcmp}. Figure \ref{memreduction} is a chart comparing the total memory utilisation running without restrictions, with Pascal policy and with the Wealth Redistribution policy using a 800MB budget.
\begin{figure}[!ht]
  \centering
    \includegraphics[width=0.7\textwidth]{r_finalfindings.png}
    \caption{Memory utilisation comparison}
    \label{memreduction}
\end{figure}
\\\\
\hspace*{1em} Figure \ref{memreduction} shows that Wealth Redistribution not only \textbf{enforces a 800MB memory limit} successfully, it also considerably \textbf{reduces the average maximum memory usage} of the scenarios as well as slightly reducing the overall average usage. The argument for deeming this policy effective is that it achieves these results without any prior knowledge of the minimum heap sizes of the scripts it manages. This makes it suitable for real scenarios where it is impractical to attempt to calculate the minimum heap size, especially in the case of websites where the host application is downloaded from the network and its behaviour is highly dependent on user input. Figure \ref{pc_reduction} shows the reduction percentages for each of the 3 measured utilisations.
\begin{figure}[!ht]
  \centering
    \includegraphics[width=0.65\textwidth]{r_averagememreduction.png}
    \caption{Memory reduction comparison (y-axis reduction percentage normalised against unrestricted run)}
    \label{pc_reduction}
\end{figure}\\
\hspace*{1em} While the memory utilisation can be greatly reduced, which brings considerable benefits such as reduced memory contention, virtual memory paging avoidance and the potential to increase the multi-programming degree, \textbf{execution time} is the \textbf{final factor} in \textbf{evaluating the feasibility} of the policy. Figure \ref{time_cmp} shows full test run execution times (all 12 scenarios) in seconds for runs with no restrictions, Pascal policy and Wealth Redistribution policy.
\begin{figure}[!ht]
  \centering
    \includegraphics[width=0.5\textwidth]{timeanalyze.png}
    \caption{Execution time comparison}
    \label{time_cmp}
\end{figure}\\\\
\hspace*{1em} \textbf{Wealth Redistribution} is approximately \textbf{20\% slower} than the unrestricted run, while \textbf{Pascal} is approximately \textbf{50\% slower}. This shows Wealth Redistribution is \textbf{2.5 times faster than Pascal} and much \textbf{more memory efficient}. The \textbf{slow down} when compared to the unrestricted running time is small enough to be considered \textbf{unnoticeable} by the user as it is only \textbf{a fraction of the optimal time}. However, there are cases when using the Wealth Redistribution policy could lead to shorter execution times compared to an unrestricted run. In situations where the maximum utilisation exceeds the available physical main memory, the operating system will compensate by using virtual memory. This process requires swapping memory pages between the main memory and permanent storage, which decreases execution speed considerably. Wealth Redistribution can be used to reduce the memory usage of applications so that they fit in memory. This would avoid paging and circumvent the implicit delays.

\section{Summary}
\hspace*{1em} Out of the 4 policies developed, only Wealth Redistribution managed to obtain a 100\% survival rate using a reasonably small memory budget. It uses the most metrics to calculate isolate welfare and makes the most use of social welfare theory to determine how to divide memory. The memory budget required to achieve complete survival is two times the sum of the minimum heaps of all managed isolates, which is reasonable considering the policy does not know the minimum heap sizes of the running scripts. The applications will experience a 20\% increase in execution when managed by this policy, but this added delay is negligible, as it is a sub-unitary fraction of the optimal time.
\chapter{Conclusion}
\hspace*{1em} Inspired by the work of Cameron et al.\cite{forseti} we have devised a memory management framework for the JavaScript ecosystem running on V8. In contrast with previous work, this project \textbf{strives for generality}, aiming to control any kind of garbage collected environment by making limit recommendations regarding the overall heap size of a program. Calculating the minimum heap size of the managed program is impractical, hence the framework uses runtime metrics collected during execution and applies social welfare theory to determine how to divide memory without causing scripts to fail. To achieve this, various memory management policies have been created and tested using bundles of standard benchmark scripts. The most effective policy was found to be Wealth Redistribution, managing to achieve a 100\% survival rate with a peak footprint reduction of 70\% and average reduction of 55\% while incurring a execution speed decrease of only 20\%. This policy would be useful in a scenario where JavaScript is used to service concurrent clients, as it helps avoid paging, increases the availability (more concurrent servicing applications per node) and does not compromise performance.
\\\\
\hspace*{1em} For the purpose of this project, the Google open source V8 engine has been modified in order to allow external control. A highly modular and extensible control framework has been built in order to profile JavaScript applications, calculate and apply memory limits and visualise feedback information about the system. Memory management policies are implemented in separate scripts and can be loaded into the framework at runtime. This allows for easy future research and extensibility without any code re-factoring or modifications. 
\section{Future Work}
\hspace*{1em} The focus of the project was to develop a working framework and an effective policy. This has led to omitting security features in favour of a less complex implementation. These features are normally part of any application that uses the network. 
\begin{enumerate}
\item \textbf{Encryption} should be used for any form of communication between the manager and the V8 processes. This would help prevent ``man-in-the-middle'' attacks.
\item \textbf{V8 Authentication} - V8 instances would use key-based authentication to prove their identity before they can be accepted as part of the system and managed by the framework. This would prevent malicious clients from disrupting the memory distribution process by creating fake clients with high memory demands.
\end{enumerate}
\hspace*{1em} Given the success of the wealth redistribution policy and the benefits it has over running concurrent JavaScript applications without any monitoring, future development should focus on improving the performance and scalability of the manager in order to make it suitable for use in the industry (distributed elastic systems). Such improvements include:
\begin{itemize}

\item \textbf{Parallel policy execution} - Policies are applied once for every machine managed by the system. No data regarding any machine other than the one being managed is necessary when applying the policy, which allows the process to be done in parallel. This would greatly improve the response time of the manager since the current implementation operates sequentially.

\item \textbf{Single plotter process} - currently the framework uses a plotter process for each isolate. This increases the processing load on the manager machine since the number of isolates can be very large. Using a single plotter process for all tracked isolates and a different live plotting library would solve this issue. Currently, if large numbers of isolates are tracked, the framework can be configured to only plot aggregated values for each machine using the command ``setPlotMode MACHINE''

\item \textbf{Automatic budget calculation} - the current implementation allows the system administrator to set a memory budget on a per-machine basis at runtime using the command line interface. In a realistic scenario, the framework would be managing between thousands and tens of thousands of machines. This makes manual setting of memory budgets impractical. An additional process could observe the overall memory utilisation and the total physical memory of each managed node and report back to the manager framework. This information can then be used to automatically calculate and set optimal machine memory budgets.

\item \textbf{Preferential Treatment} - the current version strives for optimal memory allocations that maximise the performance of every managed application. However, in a realistic setting, preferential treatment is a good way of increasing earnings. A good example for this is Amazon WebServices, where higher paying clients receive more computing power. A management policy could be devised to take preferential treatment into account and allocate more memory to applications running for higher paying clients. These clients would experience faster execution speeds than regular clients.

\item \textbf{Manage heterogeneous systems} - the framework was designed with generality in mind, hence it sets limits for the entire heap rather than for the specific spaces the V8 garbage collector maintains. In the future it could be extended to other garbage collected environments such as Java, Haskell and Python. 
\end{itemize}
More experimental work can be carried out in the hopes to devise better management policies such as:
\begin{itemize}
\item Collecting \textbf{more metrics} could provide the necessary insight to devise a policy capable of achieving a 100\% survival rate with a budget equal to the minimum heap size, or at least a smaller budget than the current best policy. Some of these metrics could be: average garbage collection time, average execution time, average frequency of garbage collection.
\item \textbf{State based policies} - all policies developed so far are stateless, they do not retain any information about previous iterations. Storing state information could help dynamically tune the policy to adapt to the specific characteristics of the environments it manages. State storage functionality is currently available in the form of a context object passed to the policy execution call.  
\end{itemize}

\section{Personal Reflection}
\hspace*{1.5em} This project has given me valuable insight into working with large scale projects such as V8. Simple development operations such as navigating the project become time consuming, often leading to spending more time navigating than developing, if the right tools are not used (such as CTags for C++ projects). Most importantly, this project has shown me, first hand, the importance of thorough documentation and applying software engineering principles. Both of which have been critical to allowing me to easily understand and modify the engine.
\\\\
\hspace*{1.5em} This work has also helped me gain confidence with regards to working with large scale projects with many other contributors. V8 comprises the largest code base that I have ever encountered, and at the beginning of the project, I had doubts about being able to make changes, never mind improve its memory footprint. By the end of this experience, my fears proved to be unfounded and the ``untouchable'' light in which I saw these projects has since disappeared. 

\section{Acknowledgements}
I would like to thank Dr. Jeremy Singer for his feedback and assistance throughout this project and Wing Li for sharing his JavaScript benchmarks.

\begin{appendices}
\chapter{Benchmark Scripts}
\label{benchmarks}
\begin{center}
\begin{tabular}{  | l | l | l | l | }
\hline  
Benchmark Script & Minimum Heap Size & Unrestricted Execution Time  & Min. Heap Execution Time\\
\hline
  Binary Tree & 196 MB & 30s & 57s \\
\hline
  Splay & 53 MB & 2s & 4s \\
\hline
  KNucleotide & 25 MB & 99s & 107s \\
\hline
  Fasta & 14 MB & 4s & 4s \\
\hline
  Binary Tree 2 & 9 MB & 12s & 24s\\
\hline
  Regexp & 7 MB & 2s & 2s \\
\hline  
  Earley-Boyer & 6 MB & 4s & 5s \\
\hline
  Crypto & 5 MB & 4s & 4s \\
\hline  
  Delta Blue & 5 MB & 2s & 2s \\
\hline  
  Navier-Stokes & 5 MB & 2s & 2s \\
\hline  
  Ray-Trace & 5 MB & 2s & 2s \\
\hline
  Richards & 5 MB & 2s & 2s \\
\hline
  RegexDNA & 5 MB& 16s & 26s \\
\hline
  Delta Blue 2 & 5 MB & 27s & 36s \\
\hline
  Richards & 5 MB & 94s & 76s \\
\hline
  N-Body & 5 MB & 10s & 10s \\
\hline
  N-Spectral Norm & 5 MB & 26s & 30s \\
\hline
  Fannkuch-Redux & 4 MB & 9s & 9s \\
\hline
  Mandelbrot & 4 MB & 8s & 8s \\
\hline
\end{tabular}
\end{center}
\chapter{Minimum Heap Size Measurements}
\label{minheapsize}
\begin{figure}[!ht]
  \centering
    \includegraphics[height=4cm]{MHCmpBinaryTree.png}
    \caption{Binary Tree Benchmark.}
\end{figure}

\begin{figure}[!ht]
  \centering
    \includegraphics[height=4cm]{MHCmpKNucleotide.png}
    \caption{KNucleotide Benchmark.}
\end{figure}

\begin{figure}[!ht]
  \centering
    \includegraphics[height=4cm]{MHCmpRegexDNA.png}
    \caption{RegexDNA Benchmark.}
\end{figure}
\chapter{CLI Commands}
\label{allcommands}
Strings in () brackets are short forms for the command.
\begin{lstlisting}
chhz [frequency in Hz(float)]
   Change the policy executor polling frequency.

chv8 [V8Id(int)]
   Change the V8 that the CLI is watching.

echo [0/1 off/on(int)]
   Toggle echo function on or off

help
   (? ) 

history
   (h ) Print command history

hz
   Get the current policy executor polling frequency

listPolicies
   (lsp ) List available policies

listScenarios
   (listscen lscen ls ) Show a listiong of all scenarios

loadConfig [configuration(str)]
   (conf lc ) Load and apply configuration file

loadpolicy [policy name(str)]
   (ldp ) Load a memory management ploicy.

machines
   (m ) Status report of all Machines.

policyname
   (p? ) Print the name of the current memory management policy.

policystats
   (ps ) Print policy status information

registryIO
   (rio r? ) Get a summary of the network usage on the registry side

run [script(str)]
   (r ) Run a JS script on the V8 the CLI is watching.

setMachineMemoryLimit [machine_id(str)] [memory_limit_in_MB(int)]
   (mlim ) Set the machine's memory budget

setmax [isolateId(int)] [heap size in bytes(int)]
   (hard ) Set hard limit reccodendation to the isolate from the V8 the CLI is watching.

setMaxPlotters [max(int)]
   (maxPlt mp ) Set maximum plotter windows

setNewMachineMemoryLimit [memory_limit_in_MB(int)]
   (nmlim ) Set the default memory budget. This is applied to machines connecting after this command is issued.

setPlotMode [mode(str)]
   (pmode pm ) Set the plot mode: NONE,MACHINE,ISOLATE,ALL

setPlotServerPort [port(int)]
   (plotport pp ) Restart the plotter server on a different port

setPlotterStartupConfig [JSON config(str)]
   (pltsconf ) Configure how the plotters behave, using a JSON string. This is applied to plotters created after this command is issued. options: makePNG(boolean) makeCSV(boolean)

snapshot [isolateId(int)]
   Take a snapshot of an isolate form the V8 the CLI is watching.

stats
   Status report of all Machines, V8 and Isolates.

suggest [isolateId(int)] [heap size in bytes(int)]
   (s soft ) Send a soft limmit reccomendation to the isolate from the V8 the CLI is watching.

switch [machineId(str)] [V8Id(int)]
   Change the machine and V8 that the CLI is watching.

testScenario [path_to_scenario(str)] [path_to_collect_in(str)]
   (scenario rs t ) Run a testing scenario

togglePlotServiceLogging [ON/OFF(str)]
   (pslog tpsl ) Toggle PlotService logging

v8s
   (v ) Status report of all Machines, V8 instances.

where
   Returns the Machine and V8 that the CLI watching. This helps simplify commands that target a single isolate.
\end{lstlisting} 
\chapter{Findings}
Survival rates for every policy:\\
\begin{tabular}{  | l | l | l | l | l | l | }
\hline  
  Run & Machine Memory Limit & Equal Share & Robin Hood & Inverse Throughput & Wealth Redistribution \\
\hline
  1 & 400MB & 91\% & 84\% & 93\% & 95\%\\
\hline
  2 & 420MB & 93\% & 92\% & 93\% & 97\%\\
\hline  
  3 & 460MB & 96\% & 90\% & 94\% & 96\%\\
\hline  
  4 & 540MB & 95\% & 92\% & 96\% & 97\%\\
\hline  
  5 & 600MB & 97\% & 90\% & 97\% & 99\%\\
\hline  
  6 & 800MB & 98\% & 88\% & 99\% & 100\%\\
\hline
  7 & 840MB & 98\% & 96\% & 99\% & 100\%\\
\hline
\end{tabular}
\\\\
Number of failed scripts out of 440 for every policy:\\
\begin{tabular}{  | l | l | l | l | l | l | }
\hline  
  Run & Machine Memory Limit & Equal Share & Robin Hood & Inverse Throughput & Wealth Redistribution \\
\hline
  1 & 400MB & 38 & 70 & 27 & 19\\
\hline
  2 & 420MB & 28 & 32 & 27 & 11\\
\hline  
  3 & 460MB & 16 & 40 & 24 & 17\\
\hline  
  4 & 540MB & 19 & 34 & 16 & 9\\
\hline  
  5 & 600MB & 9 & 42 & 12 & 4\\
\hline  
  6 & 800MB & 8 & 50 & 2 & 0\\
\hline
  7 & 840MB & 7 & 16 & 2 & 0\\
\hline
\end{tabular}
\\\\
Execution time comparison. Time format expressed as mm:ss ( m - minutes, s - seconds)\\
\begin{tabular}{  | l | l | l | l | l | l | }
\hline  
  Scenario & Normal Execution Time (minute:second) & Time with Pascal & Time with Wealth Redistribution \\
\hline
  1 & 01:22 & 02:11 & 01:44 \\
\hline
  2 & 02:34 & 04:48 & 04:07 \\
\hline  
  3 & 00:32 & 01:32 & 00:49 \\
\hline  
  4 & 00:20 & 00:21 & 00:20 \\
\hline  
  5 & 02:27 & 03:07 & 03:42 \\
\hline  
  6 & 05:28 & 10:58 & 07:10 \\
\hline
  7 & 00:03 & 00:03 & 00:03 \\
\hline
  8 & 02:04 & 02:06 & 02:14 \\
\hline
  9 & 02:14 & 02:29 & 02:40 \\
\hline
  10 & 05:24 & 06:25 & 05:31 \\
\hline
  11 & 04:38 & 07:07 & 06:44 \\
\hline
  12 & 08:31 & 09:07 & 08:39 \\
\hline
  TOTAL: & 35s & 50s & 43s\\
\hline
\end{tabular}
\chapter{Memory Utilisation Comparison}
\label{memcmp}
Groups of 3 depict total memory utilisation for each scenario in megabytes. Each group contains an unrestricted run (left), pascal run (middle) and wealth redistribution run (right).
\begin{figure}[!ht]
  \centering
    \includegraphics[width=1.0\textwidth]{1.jpg}
  \caption{Scenario 1}
\end{figure}
\begin{figure}[!ht]
  \centering
    \includegraphics[width=1.0\textwidth]{2.jpg}
  \caption{Scenario 2}
\end{figure}
\begin{figure}[!ht]
  \centering
    \includegraphics[width=1.0\textwidth]{3.jpg}
  \caption{Scenario 3}
\end{figure}
\begin{figure}[!ht]
  \centering
    \includegraphics[width=1.0\textwidth]{4.jpg}
  \caption{Scenario 4}
\end{figure}
\begin{figure}[!ht]
  \centering
    \includegraphics[width=1.0\textwidth]{5.jpg}
  \caption{Scenario 5}
\end{figure}
\begin{figure}[!ht]
  \centering
    \includegraphics[width=1.0\textwidth]{6.jpg}
  \caption{Scenario 6}
\end{figure}
\begin{figure}[!ht]
  \centering
    \includegraphics[width=1.0\textwidth]{7.jpg}
  \caption{Scenario 7}
\end{figure}
\begin{figure}[!ht]
  \centering
    \includegraphics[width=1.0\textwidth]{8.jpg}
  \caption{Scenario 8}
\end{figure}
\begin{figure}[!ht]
  \centering
    \includegraphics[width=1.0\textwidth]{9.jpg}
  \caption{Scenario 9}
\end{figure}
\begin{figure}[!ht]
  \centering
    \includegraphics[width=1.0\textwidth]{10.jpg}
  \caption{Scenario 10}
\end{figure}
\begin{figure}[!ht]
  \centering
    \includegraphics[width=1.0\textwidth]{11.jpg}
  \caption{Scenario 11}
\end{figure}
\begin{figure}[!ht]
  \centering
    \includegraphics[width=1.0\textwidth]{12.jpg}
  \caption{Scenario 12}
\end{figure}
\chapter{Test Scenarios}
\label{scenarioapx}
Blow is an example of a scenario file that instantiates two binary tree scripts.
\begin{lstlisting}[language=json,firstnumber=1]
{
  "config":{"RunWithMinHeap":false},
  "run":[{
    "program":"path/SociableJavascript/v8runner/v8wrapper.bin",
    "params":["1024","path/SociableJavascript/benchmarks/binarytree.js"],
    "instanceCount":2,
    "minHeapSize":196
  }]
}
\end{lstlisting}
The test scenarios used for the evaluation are:
\begin{center}
\begin{tabular}{|l|l|l|l|}
\hline
Scenario Name & Concurrent Scripts & Largest Minimum Heap Size & Execution Time\\
\hline
ACSA\_Fasta & 28 & 14MB & 1m 22s\\
\hline
M\_1 & 20 & 196MB & 2m 34s\\
\hline
AVSA\_BinaryTree & 2 & 196MB & 32s\\
\hline
ACSA\_Crypto & 80 & 5MB & 20s\\
\hline
M\_2 & 25 & 196MB & 2m 27s\\
\hline
M\_3 & 67 & 25MB & 5m 28s\\
\hline
AVSA\_Splay & 8 & 53MB & 3s\\
\hline
ACM\_1 & 63  & 14MB & 2m 4s\\
\hline
ACM\_2 & 71  & 14MB & 2m 14s\\
\hline
AVM\_1 & 13  & 196MB & 5m 24s\\
\hline
AVM\_2 & 27  & 25MB & 4m 38s\\
\hline
AVM\_3 & 36 & 9MB & 8m 31s\\
\hline
\end{tabular}
\end{center}

\end{appendices}

%more accurate biblio
\begin{thebibliography}{9}
\bibitem{nodejs}
NWJS Project
\url{http://nwjs.io/}
\bibitem{nwjs}
NodeJS Project
\url{https://nodejs.org/en/}
\bibitem{v8}
V8 Engine
\url{https://developers.google.com/v8/}
\bibitem{spidermk}
Spidermonkey Engine
\url{https://developer.mozilla.org/en-US/docs/Mozilla/Projects/SpiderMonkey}
\bibitem{chakra}
Chackra Engine
\url{https://github.com/Microsoft/ChakraCore}
\bibitem{jank}
Jank definition,
\url{https://developers.google.com/web/fundamentals/performance/rendering/?hl=en}
\bibitem{powderplayer}
Powder Player JavaScript Desktop Application
\url{https://github.com/jaruba/PowderPlayer}
\bibitem{whatsap}
Whatsapp JavaScript Desktop Application
\url{https://web.whatsapp.com/}
\bibitem{messenger}
Facebook Messenger JavaScript Desktop Application
\url{http://messengerfordesktop.com/}
\bibitem{devkit}
Devkit JavaScript Desktop Application
\url{https://github.com/printhom/devkit-core}
\bibitem{wunderlist}
Wunderlist JavaScript Desktop Application
\url{https://www.wunderlist.com/download/}
\bibitem{tycoongame}
Tycoon JavaScript Desktop Game
\url{http://www.greenheartgames.com/app/game-dev-tycoon/}
\bibitem{forseti}
Callum Cameron, Jeremy Singer, David Vengerov
\emph{The Judgment of Forseti: Economic Utility for Dynamic Heap Sizing of Multiple Runtimes}
\bibitem{welfareeconomics}
Deardorff, Alan V. (2014), "Welfare economics", Deardorffs' Glossary of International Economics \url{http://www-personal.umich.edu/~alandear/glossary/w.html#WelfareEconomics}
\bibitem{socialwelfarefunction}
Amartya K. Sen, 1970 [1984], Collective Choice and Social Welfare, ch. 3, "Collective Rationality." p. 33, and ch. 3*, "Social Welfare Functions." \url{http://www.citeulike.org/user/rlai/article/681900}
\bibitem{jsgrandpa}
Naomi Hamilton, “The A–Z of Programming Languages: JavaScript,” Computerworld, July 30, 2008, \url{http://bit.ly/1lKldIe}
\bibitem{jsdaddy}
 Paul Krill, “JavaScript Creator Ponders Past, Future,” InfoWorld, June 23, 2008, http://bit.ly/1lKlpXO; Brendan Eich, “A Brief History of JavaScript,” July 21, 2010, \url{http://bit.ly/1lKkI0M}
\bibitem{github}
GitHub, version control service based on git \hspace*{1em} \url{https://github.com/}
\bibitem{githut}
GitHut, Statistics for GitHub \hspace*{1em} \url{http://githut.info/}
\bibitem{monk}
RedMonk Programming Language Ranking
\url{https://redmonk.com/sogrady/category/programming-languages/}
\bibitem{gcpaper}
Paul R Wilson. Uniprocessor garbage collection techniques. In Memory Management, Springer, 1992.
\bibitem{ergonomics}
Java Runtime Heap Resizing, Ergonomics,
\url{https://docs.oracle.com/javase/8/docs/technotes/guides/vm/gctuning/ergonomics.html}
\bibitem{v8sizebug}
V8 Maximum Memory Amount Per Isolate
\url{https://bugs.chromium.org/p/v8/issues/detail?id=847}
\bibitem{diminishreturns}
Page 3, Microeconomic Theory,\emph{The Judgment of Forseti: Economic Utility for Dynamic Heap Sizing of Multiple Runtimes}, Callum Cameron, Jeremy Singer, David Vengerov
\bibitem{v8gctour}
A tour of the V8 garbage collector,
\url{http://jayconrod.com/posts/55/a-tour-of-v8-garbage-collection}
\bibitem{matplotlib}
Matplotlib, plotting library for python
\url{http://matplotlib.org/}
\bibitem{gini}
Gini, C. (1909). "Concentration and dependency ratios" (in Italian). English translation in Rivista di Politica Economica, 87 (1997), 769–789.
\bibitem{cheney}
Cheney Algorithm,
\url{https://en.wikipedia.org/wiki/Cheney\%27s_algorithm}
\end{thebibliography}
\end{document}